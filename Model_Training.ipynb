{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b25eb04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms ,models\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from spectral import envi, principal_components\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import albumentations as A\n",
    "import shutil\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaae90d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder('aug_processed_data', transform=transform)\n",
    "\n",
    "# Train/val split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39fbb19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 32 * 32, 64)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))   # 64x64\n",
    "        x = self.pool(F.relu(self.conv2(x)))   # 32x32\n",
    "        x = x.view(-1, 32 * 32 * 32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da11de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Approch 2\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Dropout with 50% probability\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * 32 * 32, 64)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))   # 128->64\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))   # 64->32\n",
    "\n",
    "        x = x.view(-1, 32 * 32 * 32)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))            # Dropout before fc2\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "946f0ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # weight_decay added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "043a7e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7209358215332031\n",
      "Batch loss: 3.0623459815979004\n",
      "Batch loss: 5.982723236083984\n",
      "Batch loss: 5.07076358795166\n",
      "Batch loss: 5.961067199707031\n",
      "Epoch 1 | Train Loss: 4.1596 | Train Acc: 56.88% | Val Loss: 0.6107 | Val Acc: 67.50%\n",
      "Batch loss: 4.462386608123779\n",
      "Batch loss: 2.5849502086639404\n",
      "Batch loss: 3.0387144088745117\n",
      "Batch loss: 1.0572116374969482\n",
      "Batch loss: 1.0128306150436401\n",
      "Epoch 2 | Train Loss: 2.4312 | Train Acc: 73.12% | Val Loss: 0.9145 | Val Acc: 77.50%\n",
      "Batch loss: 1.0918093919754028\n",
      "Batch loss: 1.1405127048492432\n",
      "Batch loss: 0.6669681072235107\n",
      "Batch loss: 0.6395519971847534\n",
      "Batch loss: 1.911686897277832\n",
      "Epoch 3 | Train Loss: 1.0901 | Train Acc: 80.62% | Val Loss: 0.4056 | Val Acc: 87.50%\n",
      "Batch loss: 0.2729165554046631\n",
      "Batch loss: 0.13588377833366394\n",
      "Batch loss: 0.2925509512424469\n",
      "Batch loss: 0.33915093541145325\n",
      "Batch loss: 0.24996091425418854\n",
      "Epoch 4 | Train Loss: 0.2581 | Train Acc: 91.25% | Val Loss: 0.6128 | Val Acc: 77.50%\n",
      "Batch loss: 0.058132294565439224\n",
      "Batch loss: 0.12155303359031677\n",
      "Batch loss: 0.29198378324508667\n",
      "Batch loss: 0.3382044732570648\n",
      "Batch loss: 0.35649141669273376\n",
      "Epoch 5 | Train Loss: 0.2333 | Train Acc: 91.88% | Val Loss: 0.3326 | Val Acc: 90.00%\n",
      "Batch loss: 0.27403131127357483\n",
      "Batch loss: 0.133191779255867\n",
      "Batch loss: 0.3658737540245056\n",
      "Batch loss: 0.1806233823299408\n",
      "Batch loss: 0.1036369577050209\n",
      "Epoch 6 | Train Loss: 0.2115 | Train Acc: 89.38% | Val Loss: 0.2819 | Val Acc: 90.00%\n",
      "Batch loss: 0.11479851603507996\n",
      "Batch loss: 0.1607678234577179\n",
      "Batch loss: 0.1090618222951889\n",
      "Batch loss: 0.15133091807365417\n",
      "Batch loss: 0.22960630059242249\n",
      "Epoch 7 | Train Loss: 0.1531 | Train Acc: 93.12% | Val Loss: 0.2666 | Val Acc: 92.50%\n",
      "Batch loss: 0.08273877203464508\n",
      "Batch loss: 0.15400508046150208\n",
      "Batch loss: 0.1046091765165329\n",
      "Batch loss: 0.15935194492340088\n",
      "Batch loss: 0.062121257185935974\n",
      "Epoch 8 | Train Loss: 0.1126 | Train Acc: 95.62% | Val Loss: 0.1991 | Val Acc: 92.50%\n",
      "Batch loss: 0.11265318840742111\n",
      "Batch loss: 0.0668315663933754\n",
      "Batch loss: 0.12109597772359848\n",
      "Batch loss: 0.09810207039117813\n",
      "Batch loss: 0.08764436841011047\n",
      "Epoch 9 | Train Loss: 0.0973 | Train Acc: 93.75% | Val Loss: 0.1635 | Val Acc: 92.50%\n",
      "Batch loss: 0.23055997490882874\n",
      "Batch loss: 0.07751250267028809\n",
      "Batch loss: 0.058417100459337234\n",
      "Batch loss: 0.0883803591132164\n",
      "Batch loss: 0.130914568901062\n",
      "Epoch 10 | Train Loss: 0.1172 | Train Acc: 95.62% | Val Loss: 0.1900 | Val Acc: 90.00%\n",
      "Batch loss: 0.06054864078760147\n",
      "Batch loss: 0.039395228028297424\n",
      "Batch loss: 0.05994019657373428\n",
      "Batch loss: 0.0436612032353878\n",
      "Batch loss: 0.06558889895677567\n",
      "Epoch 11 | Train Loss: 0.0538 | Train Acc: 98.12% | Val Loss: 0.3744 | Val Acc: 90.00%\n",
      "Batch loss: 0.17290529608726501\n",
      "Batch loss: 0.05544243007898331\n",
      "Batch loss: 0.2032797634601593\n",
      "Batch loss: 0.07501521706581116\n",
      "Batch loss: 0.07833126932382584\n",
      "Epoch 12 | Train Loss: 0.1170 | Train Acc: 96.25% | Val Loss: 0.3559 | Val Acc: 90.00%\n",
      "Batch loss: 0.03980959206819534\n",
      "Batch loss: 0.024825800210237503\n",
      "Batch loss: 0.09396811574697495\n",
      "Batch loss: 0.07520513981580734\n",
      "Batch loss: 0.04655824229121208\n",
      "Epoch 13 | Train Loss: 0.0561 | Train Acc: 96.25% | Val Loss: 0.2698 | Val Acc: 92.50%\n",
      "Batch loss: 0.02239428646862507\n",
      "Batch loss: 0.03267721086740494\n",
      "Batch loss: 0.09018010646104813\n",
      "Batch loss: 0.05076535791158676\n",
      "Batch loss: 0.005968828685581684\n",
      "Epoch 14 | Train Loss: 0.0404 | Train Acc: 99.38% | Val Loss: 0.2438 | Val Acc: 95.00%\n",
      "Batch loss: 0.05707159638404846\n",
      "Batch loss: 0.02401559054851532\n",
      "Batch loss: 0.042889345437288284\n",
      "Batch loss: 0.04959767684340477\n",
      "Batch loss: 0.10045140236616135\n",
      "Epoch 15 | Train Loss: 0.0548 | Train Acc: 98.12% | Val Loss: 0.2616 | Val Acc: 92.50%\n",
      "Batch loss: 0.029389988631010056\n",
      "Batch loss: 0.03971025347709656\n",
      "Batch loss: 0.023821327835321426\n",
      "Batch loss: 0.030795561149716377\n",
      "Batch loss: 0.01957060396671295\n",
      "Epoch 16 | Train Loss: 0.0287 | Train Acc: 99.38% | Val Loss: 0.2993 | Val Acc: 95.00%\n",
      "Batch loss: 0.030260886996984482\n",
      "Batch loss: 0.09799309819936752\n",
      "Batch loss: 0.0144385676831007\n",
      "Batch loss: 0.013853419572114944\n",
      "Batch loss: 0.029680699110031128\n",
      "Epoch 17 | Train Loss: 0.0372 | Train Acc: 98.75% | Val Loss: 0.2675 | Val Acc: 95.00%\n",
      "Batch loss: 0.02253870666027069\n",
      "Batch loss: 0.034846231341362\n",
      "Batch loss: 0.02232169173657894\n",
      "Batch loss: 0.059996940195560455\n",
      "Batch loss: 0.021083150058984756\n",
      "Epoch 18 | Train Loss: 0.0322 | Train Acc: 99.38% | Val Loss: 0.3139 | Val Acc: 95.00%\n",
      "Batch loss: 0.0413728728890419\n",
      "Batch loss: 0.008873116225004196\n",
      "Batch loss: 0.0023646699264645576\n",
      "Batch loss: 0.09771527349948883\n",
      "Batch loss: 0.018679488450288773\n",
      "Epoch 19 | Train Loss: 0.0338 | Train Acc: 99.38% | Val Loss: 0.3539 | Val Acc: 95.00%\n",
      "Batch loss: 0.018337631598114967\n",
      "Batch loss: 0.041796356439590454\n",
      "Batch loss: 0.04719683527946472\n",
      "Batch loss: 0.011200735345482826\n",
      "Batch loss: 0.011112402193248272\n",
      "Epoch 20 | Train Loss: 0.0259 | Train Acc: 98.75% | Val Loss: 0.3057 | Val Acc: 92.50%\n",
      "Batch loss: 0.0508512407541275\n",
      "Batch loss: 0.03749878332018852\n",
      "Batch loss: 0.07483847439289093\n",
      "Batch loss: 0.08728809654712677\n",
      "Batch loss: 0.0015642590587958694\n",
      "Epoch 21 | Train Loss: 0.0504 | Train Acc: 95.62% | Val Loss: 0.2702 | Val Acc: 95.00%\n",
      "Batch loss: 0.032999638468027115\n",
      "Batch loss: 0.0050769406370818615\n",
      "Batch loss: 0.06066595017910004\n",
      "Batch loss: 0.005689590238034725\n",
      "Batch loss: 0.02662728540599346\n",
      "Epoch 22 | Train Loss: 0.0262 | Train Acc: 99.38% | Val Loss: 0.3016 | Val Acc: 92.50%\n",
      "Batch loss: 0.08127833902835846\n",
      "Batch loss: 0.045516978949308395\n",
      "Batch loss: 0.052952732890844345\n",
      "Batch loss: 0.034412648528814316\n",
      "Batch loss: 0.011154771782457829\n",
      "Epoch 23 | Train Loss: 0.0451 | Train Acc: 97.50% | Val Loss: 0.2562 | Val Acc: 92.50%\n",
      "Batch loss: 0.012887056916952133\n",
      "Batch loss: 0.03715333342552185\n",
      "Batch loss: 0.006695277523249388\n",
      "Batch loss: 0.024745851755142212\n",
      "Batch loss: 0.016274023801088333\n",
      "Epoch 24 | Train Loss: 0.0196 | Train Acc: 100.00% | Val Loss: 0.2592 | Val Acc: 92.50%\n",
      "Batch loss: 0.059018220752477646\n",
      "Batch loss: 0.010552775114774704\n",
      "Batch loss: 0.007326232269406319\n",
      "Batch loss: 0.011264696717262268\n",
      "Batch loss: 0.08440819382667542\n",
      "Epoch 25 | Train Loss: 0.0345 | Train Acc: 98.12% | Val Loss: 0.3073 | Val Acc: 92.50%\n",
      "Batch loss: 0.012095652520656586\n",
      "Batch loss: 0.018889205530285835\n",
      "Batch loss: 0.025260906666517258\n",
      "Batch loss: 0.004342300351709127\n",
      "Batch loss: 0.011545415967702866\n",
      "Epoch 26 | Train Loss: 0.0144 | Train Acc: 100.00% | Val Loss: 0.4264 | Val Acc: 95.00%\n",
      "Batch loss: 0.02723032794892788\n",
      "Batch loss: 0.01783849112689495\n",
      "Batch loss: 0.016487816348671913\n",
      "Batch loss: 0.018472721800208092\n",
      "Batch loss: 0.023337261751294136\n",
      "Epoch 27 | Train Loss: 0.0207 | Train Acc: 99.38% | Val Loss: 0.4807 | Val Acc: 95.00%\n",
      "Batch loss: 0.013601240701973438\n",
      "Batch loss: 0.009061592631042004\n",
      "Batch loss: 0.019376806914806366\n",
      "Batch loss: 0.0019222958944737911\n",
      "Batch loss: 0.016875626519322395\n",
      "Epoch 28 | Train Loss: 0.0122 | Train Acc: 100.00% | Val Loss: 0.4854 | Val Acc: 95.00%\n",
      "Batch loss: 0.025746557861566544\n",
      "Batch loss: 0.028514748439192772\n",
      "Batch loss: 0.005568536929786205\n",
      "Batch loss: 0.004776593763381243\n",
      "Batch loss: 0.04700294882059097\n",
      "Epoch 29 | Train Loss: 0.0223 | Train Acc: 98.12% | Val Loss: 0.5076 | Val Acc: 92.50%\n",
      "Batch loss: 0.029764220118522644\n",
      "Batch loss: 0.021663229912519455\n",
      "Batch loss: 0.02499573677778244\n",
      "Batch loss: 0.008365388959646225\n",
      "Batch loss: 0.06009594351053238\n",
      "Epoch 30 | Train Loss: 0.0290 | Train Acc: 100.00% | Val Loss: 0.6089 | Val Acc: 95.00%\n",
      "Batch loss: 0.005581836216151714\n",
      "Batch loss: 0.00843597762286663\n",
      "Batch loss: 0.004522193688899279\n",
      "Batch loss: 0.004201162606477737\n",
      "Batch loss: 0.01409461721777916\n",
      "Epoch 31 | Train Loss: 0.0074 | Train Acc: 100.00% | Val Loss: 0.5084 | Val Acc: 95.00%\n",
      "Batch loss: 0.03975294902920723\n",
      "Batch loss: 0.028115050867199898\n",
      "Batch loss: 0.008250702172517776\n",
      "Batch loss: 0.03890576213598251\n",
      "Batch loss: 0.011876250617206097\n",
      "Epoch 32 | Train Loss: 0.0254 | Train Acc: 98.12% | Val Loss: 0.3155 | Val Acc: 92.50%\n",
      "Batch loss: 0.0032931342720985413\n",
      "Batch loss: 0.0339534729719162\n",
      "Batch loss: 0.008848884142935276\n",
      "Batch loss: 0.02332005836069584\n",
      "Batch loss: 0.009115596301853657\n",
      "Epoch 33 | Train Loss: 0.0157 | Train Acc: 99.38% | Val Loss: 0.5128 | Val Acc: 92.50%\n",
      "Batch loss: 0.025570537894964218\n",
      "Batch loss: 0.027266623452305794\n",
      "Batch loss: 0.03382931649684906\n",
      "Batch loss: 0.005230787675827742\n",
      "Batch loss: 0.007829036563634872\n",
      "Epoch 34 | Train Loss: 0.0199 | Train Acc: 100.00% | Val Loss: 0.4343 | Val Acc: 92.50%\n",
      "Batch loss: 0.02917230688035488\n",
      "Batch loss: 0.023760275915265083\n",
      "Batch loss: 0.01015368290245533\n",
      "Batch loss: 0.011637135408818722\n",
      "Batch loss: 0.014061437919735909\n",
      "Epoch 35 | Train Loss: 0.0178 | Train Acc: 100.00% | Val Loss: 0.3324 | Val Acc: 95.00%\n",
      "Batch loss: 0.014719600789248943\n",
      "Batch loss: 0.01283825933933258\n",
      "Batch loss: 0.007054712623357773\n",
      "Batch loss: 0.015146722085773945\n",
      "Batch loss: 0.017510581761598587\n",
      "Epoch 36 | Train Loss: 0.0135 | Train Acc: 100.00% | Val Loss: 0.3214 | Val Acc: 95.00%\n",
      "Batch loss: 0.03286956623196602\n",
      "Batch loss: 0.016477134078741074\n",
      "Batch loss: 0.0036268560215830803\n",
      "Batch loss: 0.0023142274003475904\n",
      "Batch loss: 0.05827091634273529\n",
      "Epoch 37 | Train Loss: 0.0227 | Train Acc: 98.75% | Val Loss: 0.4700 | Val Acc: 95.00%\n",
      "Batch loss: 0.004520762246102095\n",
      "Batch loss: 0.0033728231210261583\n",
      "Batch loss: 0.03604874759912491\n",
      "Batch loss: 0.007004472427070141\n",
      "Batch loss: 0.0029021489899605513\n",
      "Epoch 38 | Train Loss: 0.0108 | Train Acc: 99.38% | Val Loss: 1.0569 | Val Acc: 87.50%\n",
      "Batch loss: 0.009413539431989193\n",
      "Batch loss: 0.018640916794538498\n",
      "Batch loss: 0.007925081998109818\n",
      "Batch loss: 0.005062678828835487\n",
      "Batch loss: 0.00719236396253109\n",
      "Epoch 39 | Train Loss: 0.0096 | Train Acc: 100.00% | Val Loss: 1.1525 | Val Acc: 90.00%\n",
      "Batch loss: 0.0010770963272079825\n",
      "Batch loss: 0.06114399805665016\n",
      "Batch loss: 0.016074752435088158\n",
      "Batch loss: 0.016730526462197304\n",
      "Batch loss: 0.015544719062745571\n",
      "Epoch 40 | Train Loss: 0.0221 | Train Acc: 99.38% | Val Loss: 0.5748 | Val Acc: 95.00%\n",
      "Batch loss: 0.0032535074278712273\n",
      "Batch loss: 0.021077945828437805\n",
      "Batch loss: 0.007663253229111433\n",
      "Batch loss: 0.004183480050414801\n",
      "Batch loss: 0.007468007504940033\n",
      "Epoch 41 | Train Loss: 0.0087 | Train Acc: 100.00% | Val Loss: 0.3641 | Val Acc: 92.50%\n",
      "Batch loss: 0.0057911984622478485\n",
      "Batch loss: 0.0999031588435173\n",
      "Batch loss: 0.008242344483733177\n",
      "Batch loss: 0.00105049810372293\n",
      "Batch loss: 0.028845639899373055\n",
      "Epoch 42 | Train Loss: 0.0288 | Train Acc: 99.38% | Val Loss: 0.3281 | Val Acc: 95.00%\n",
      "Batch loss: 0.020729264244437218\n",
      "Batch loss: 0.002956202020868659\n",
      "Batch loss: 0.022849325090646744\n",
      "Batch loss: 0.016565721482038498\n",
      "Batch loss: 0.0015362317208200693\n",
      "Epoch 43 | Train Loss: 0.0129 | Train Acc: 100.00% | Val Loss: 0.3303 | Val Acc: 95.00%\n",
      "Batch loss: 0.005933284759521484\n",
      "Batch loss: 0.004802084062248468\n",
      "Batch loss: 0.0030139321461319923\n",
      "Batch loss: 0.005323207937180996\n",
      "Batch loss: 0.001236072275787592\n",
      "Epoch 44 | Train Loss: 0.0041 | Train Acc: 100.00% | Val Loss: 0.3599 | Val Acc: 95.00%\n",
      "Batch loss: 0.014990841038525105\n",
      "Batch loss: 0.001976653002202511\n",
      "Batch loss: 0.0037075220607221127\n",
      "Batch loss: 0.003310734871774912\n",
      "Batch loss: 0.003528702538460493\n",
      "Epoch 45 | Train Loss: 0.0055 | Train Acc: 100.00% | Val Loss: 0.3648 | Val Acc: 95.00%\n",
      "Batch loss: 0.014911225065588951\n",
      "Batch loss: 0.009330201894044876\n",
      "Batch loss: 0.009695738554000854\n",
      "Batch loss: 0.02251673862338066\n",
      "Batch loss: 0.006936625111848116\n",
      "Epoch 46 | Train Loss: 0.0127 | Train Acc: 100.00% | Val Loss: 0.4841 | Val Acc: 95.00%\n",
      "Batch loss: 0.0182387325912714\n",
      "Batch loss: 0.0003498210571706295\n",
      "Batch loss: 0.01570715755224228\n",
      "Batch loss: 0.0031083603389561176\n",
      "Batch loss: 0.006570287514477968\n",
      "Epoch 47 | Train Loss: 0.0088 | Train Acc: 100.00% | Val Loss: 0.5276 | Val Acc: 95.00%\n",
      "Batch loss: 0.006221499294042587\n",
      "Batch loss: 0.015388520434498787\n",
      "Batch loss: 0.0010208599269390106\n",
      "Batch loss: 0.02779044769704342\n",
      "Batch loss: 0.005957811139523983\n",
      "Epoch 48 | Train Loss: 0.0113 | Train Acc: 99.38% | Val Loss: 0.5032 | Val Acc: 92.50%\n",
      "Batch loss: 0.0006888210773468018\n",
      "Batch loss: 0.02845848724246025\n",
      "Batch loss: 0.001570236636325717\n",
      "Batch loss: 0.0019194255582988262\n",
      "Batch loss: 0.01231437362730503\n",
      "Epoch 49 | Train Loss: 0.0090 | Train Acc: 99.38% | Val Loss: 0.5363 | Val Acc: 92.50%\n",
      "Batch loss: 0.005842193495482206\n",
      "Batch loss: 0.0007776643615216017\n",
      "Batch loss: 0.0025598006322979927\n",
      "Batch loss: 0.002904321067035198\n",
      "Batch loss: 0.014745921827852726\n",
      "Epoch 50 | Train Loss: 0.0054 | Train Acc: 100.00% | Val Loss: 0.5563 | Val Acc: 95.00%\n",
      "Batch loss: 0.002638133242726326\n",
      "Batch loss: 0.004002481698989868\n",
      "Batch loss: 0.0009937979048117995\n",
      "Batch loss: 0.003610955085605383\n",
      "Batch loss: 0.0010639019310474396\n",
      "Epoch 51 | Train Loss: 0.0025 | Train Acc: 100.00% | Val Loss: 0.6136 | Val Acc: 92.50%\n",
      "Batch loss: 0.02470236085355282\n",
      "Batch loss: 0.007330235559493303\n",
      "Batch loss: 0.0034067491069436073\n",
      "Batch loss: 0.0029927438590675592\n",
      "Batch loss: 0.004058659076690674\n",
      "Epoch 52 | Train Loss: 0.0085 | Train Acc: 99.38% | Val Loss: 0.6446 | Val Acc: 92.50%\n",
      "Batch loss: 0.003779314225539565\n",
      "Batch loss: 0.03947265073657036\n",
      "Batch loss: 0.01747763901948929\n",
      "Batch loss: 0.0002557501138653606\n",
      "Batch loss: 0.0016749349888414145\n",
      "Epoch 53 | Train Loss: 0.0125 | Train Acc: 99.38% | Val Loss: 0.7421 | Val Acc: 92.50%\n",
      "Batch loss: 0.0023108702152967453\n",
      "Batch loss: 0.0004533766768872738\n",
      "Batch loss: 0.002105342224240303\n",
      "Batch loss: 0.03245013952255249\n",
      "Batch loss: 0.0006239489302970469\n",
      "Epoch 54 | Train Loss: 0.0076 | Train Acc: 100.00% | Val Loss: 0.7676 | Val Acc: 95.00%\n",
      "Batch loss: 0.0007101786904968321\n",
      "Batch loss: 0.0017780245980247855\n",
      "Batch loss: 0.005530280061066151\n",
      "Batch loss: 0.012521379627287388\n",
      "Batch loss: 0.0037086731754243374\n",
      "Epoch 55 | Train Loss: 0.0048 | Train Acc: 100.00% | Val Loss: 0.6682 | Val Acc: 92.50%\n",
      "Batch loss: 0.022230247035622597\n",
      "Batch loss: 0.000410014356020838\n",
      "Batch loss: 0.006867288611829281\n",
      "Batch loss: 0.0008993892115540802\n",
      "Batch loss: 0.00034759199479594827\n",
      "Epoch 56 | Train Loss: 0.0062 | Train Acc: 100.00% | Val Loss: 0.5235 | Val Acc: 92.50%\n",
      "Batch loss: 0.0005566185573115945\n",
      "Batch loss: 0.029786039143800735\n",
      "Batch loss: 0.0017288021044805646\n",
      "Batch loss: 0.038690198212862015\n",
      "Batch loss: 0.0006636437028646469\n",
      "Epoch 57 | Train Loss: 0.0143 | Train Acc: 99.38% | Val Loss: 0.4350 | Val Acc: 95.00%\n",
      "Batch loss: 0.009172349236905575\n",
      "Batch loss: 0.0017716214060783386\n",
      "Batch loss: 0.002089771907776594\n",
      "Batch loss: 0.0020049656741321087\n",
      "Batch loss: 0.017147423699498177\n",
      "Epoch 58 | Train Loss: 0.0064 | Train Acc: 100.00% | Val Loss: 0.3646 | Val Acc: 95.00%\n",
      "Batch loss: 0.022230662405490875\n",
      "Batch loss: 6.294233753578737e-05\n",
      "Batch loss: 0.021305575966835022\n",
      "Batch loss: 0.0006546028307639062\n",
      "Batch loss: 0.0027284666430205107\n",
      "Epoch 59 | Train Loss: 0.0094 | Train Acc: 100.00% | Val Loss: 0.4904 | Val Acc: 92.50%\n",
      "Batch loss: 0.0027565627824515104\n",
      "Batch loss: 0.012199347838759422\n",
      "Batch loss: 0.013887607492506504\n",
      "Batch loss: 0.0029140696860849857\n",
      "Batch loss: 0.0011429344303905964\n",
      "Epoch 60 | Train Loss: 0.0066 | Train Acc: 100.00% | Val Loss: 0.5491 | Val Acc: 95.00%\n",
      "Batch loss: 0.024805646389722824\n",
      "Batch loss: 0.0036717341281473637\n",
      "Batch loss: 0.0002813841274473816\n",
      "Batch loss: 0.0056961472146213055\n",
      "Batch loss: 0.008633206598460674\n",
      "Epoch 61 | Train Loss: 0.0086 | Train Acc: 100.00% | Val Loss: 0.5796 | Val Acc: 92.50%\n",
      "Batch loss: 0.021923759952187538\n",
      "Batch loss: 0.058682844042778015\n",
      "Batch loss: 0.0011779015185311437\n",
      "Batch loss: 0.003445951733738184\n",
      "Batch loss: 0.0045974645763635635\n",
      "Epoch 62 | Train Loss: 0.0180 | Train Acc: 99.38% | Val Loss: 0.4987 | Val Acc: 92.50%\n",
      "Batch loss: 0.012259709648787975\n",
      "Batch loss: 0.00016721019346732646\n",
      "Batch loss: 0.0009000844438560307\n",
      "Batch loss: 0.02973737195134163\n",
      "Batch loss: 0.061782412230968475\n",
      "Epoch 63 | Train Loss: 0.0210 | Train Acc: 98.75% | Val Loss: 0.4909 | Val Acc: 95.00%\n",
      "Batch loss: 0.01361388061195612\n",
      "Batch loss: 0.005021765828132629\n",
      "Batch loss: 0.018850546330213547\n",
      "Batch loss: 0.0010238313116133213\n",
      "Batch loss: 0.021349284797906876\n",
      "Epoch 64 | Train Loss: 0.0120 | Train Acc: 100.00% | Val Loss: 0.6510 | Val Acc: 95.00%\n",
      "Batch loss: 0.03032613918185234\n",
      "Batch loss: 0.005513628013432026\n",
      "Batch loss: 0.0002020796382566914\n",
      "Batch loss: 0.000788091856520623\n",
      "Batch loss: 0.0005966101307421923\n",
      "Epoch 65 | Train Loss: 0.0075 | Train Acc: 100.00% | Val Loss: 0.6963 | Val Acc: 95.00%\n",
      "Batch loss: 0.0009180019842460752\n",
      "Batch loss: 0.00927562452852726\n",
      "Batch loss: 0.0008220637100748718\n",
      "Batch loss: 0.00012900539149995893\n",
      "Batch loss: 0.0019390403758734465\n",
      "Epoch 66 | Train Loss: 0.0026 | Train Acc: 100.00% | Val Loss: 0.8357 | Val Acc: 95.00%\n",
      "Batch loss: 0.0017788283294066787\n",
      "Batch loss: 0.03247242420911789\n",
      "Batch loss: 0.029601195827126503\n",
      "Batch loss: 0.003760240040719509\n",
      "Batch loss: 0.002369556576013565\n",
      "Epoch 67 | Train Loss: 0.0140 | Train Acc: 99.38% | Val Loss: 0.7996 | Val Acc: 95.00%\n",
      "Batch loss: 0.005128649529069662\n",
      "Batch loss: 0.006007161922752857\n",
      "Batch loss: 0.02274485118687153\n",
      "Batch loss: 0.0034021856263279915\n",
      "Batch loss: 0.0029693651013076305\n",
      "Epoch 68 | Train Loss: 0.0081 | Train Acc: 100.00% | Val Loss: 0.7119 | Val Acc: 95.00%\n",
      "Batch loss: 0.0005375472246669233\n",
      "Batch loss: 0.000882382330019027\n",
      "Batch loss: 0.0012370652984827757\n",
      "Batch loss: 0.006144429557025433\n",
      "Batch loss: 0.0009690123843029141\n",
      "Epoch 69 | Train Loss: 0.0020 | Train Acc: 100.00% | Val Loss: 0.5904 | Val Acc: 95.00%\n",
      "Batch loss: 0.0015939323930069804\n",
      "Batch loss: 0.05120564252138138\n",
      "Batch loss: 0.012496133334934711\n",
      "Batch loss: 0.001258563599549234\n",
      "Batch loss: 0.0056559015065431595\n",
      "Epoch 70 | Train Loss: 0.0144 | Train Acc: 99.38% | Val Loss: 0.5339 | Val Acc: 92.50%\n",
      "Batch loss: 0.0005967355682514608\n",
      "Batch loss: 0.0004964289255440235\n",
      "Batch loss: 0.00467561325058341\n",
      "Batch loss: 0.0010893032886087894\n",
      "Batch loss: 0.0043895966373384\n",
      "Epoch 71 | Train Loss: 0.0022 | Train Acc: 100.00% | Val Loss: 0.5454 | Val Acc: 95.00%\n",
      "Batch loss: 0.0023380364291369915\n",
      "Batch loss: 0.00015242777590174228\n",
      "Batch loss: 0.0060861301608383656\n",
      "Batch loss: 0.005890348460525274\n",
      "Batch loss: 7.42269039619714e-05\n",
      "Epoch 72 | Train Loss: 0.0029 | Train Acc: 100.00% | Val Loss: 0.5315 | Val Acc: 95.00%\n",
      "Batch loss: 0.0006155624287202954\n",
      "Batch loss: 0.02312093786895275\n",
      "Batch loss: 0.007516816258430481\n",
      "Batch loss: 0.005260416306555271\n",
      "Batch loss: 0.019396524876356125\n",
      "Epoch 73 | Train Loss: 0.0112 | Train Acc: 99.38% | Val Loss: 0.6563 | Val Acc: 90.00%\n",
      "Batch loss: 0.0021068230271339417\n",
      "Batch loss: 0.0009827385656535625\n",
      "Batch loss: 0.00030921888537704945\n",
      "Batch loss: 0.0007328994106501341\n",
      "Batch loss: 0.010730958543717861\n",
      "Epoch 74 | Train Loss: 0.0030 | Train Acc: 100.00% | Val Loss: 0.6437 | Val Acc: 92.50%\n",
      "Batch loss: 0.0002257602900499478\n",
      "Batch loss: 0.03031378984451294\n",
      "Batch loss: 0.00013278870028443635\n",
      "Batch loss: 0.004363786894828081\n",
      "Batch loss: 0.0004468953993637115\n",
      "Epoch 75 | Train Loss: 0.0071 | Train Acc: 100.00% | Val Loss: 0.4892 | Val Acc: 92.50%\n",
      "Batch loss: 0.06377141177654266\n",
      "Batch loss: 0.00010563051182543859\n",
      "Batch loss: 0.02870640903711319\n",
      "Batch loss: 0.006876789964735508\n",
      "Batch loss: 0.0021191886626183987\n",
      "Epoch 76 | Train Loss: 0.0203 | Train Acc: 98.75% | Val Loss: 0.6654 | Val Acc: 95.00%\n",
      "Batch loss: 0.0064872256480157375\n",
      "Batch loss: 0.0008967799949459732\n",
      "Batch loss: 0.0005273035494610667\n",
      "Batch loss: 0.0007260102429427207\n",
      "Batch loss: 0.017903555184602737\n",
      "Epoch 77 | Train Loss: 0.0053 | Train Acc: 100.00% | Val Loss: 0.7855 | Val Acc: 92.50%\n",
      "Batch loss: 0.03492473438382149\n",
      "Batch loss: 0.016207197681069374\n",
      "Batch loss: 0.06780711561441422\n",
      "Batch loss: 0.00023235013941302896\n",
      "Batch loss: 0.021894773468375206\n",
      "Epoch 78 | Train Loss: 0.0282 | Train Acc: 98.75% | Val Loss: 0.5241 | Val Acc: 92.50%\n",
      "Batch loss: 0.020847180858254433\n",
      "Batch loss: 0.0002114274975610897\n",
      "Batch loss: 0.0016898419708013535\n",
      "Batch loss: 4.916265243082307e-05\n",
      "Batch loss: 0.05352820083498955\n",
      "Epoch 79 | Train Loss: 0.0153 | Train Acc: 99.38% | Val Loss: 0.3925 | Val Acc: 95.00%\n",
      "Batch loss: 0.004876425024122\n",
      "Batch loss: 0.0030016168020665646\n",
      "Batch loss: 0.0025420153979212046\n",
      "Batch loss: 0.021218497306108475\n",
      "Batch loss: 0.0007206351729109883\n",
      "Epoch 80 | Train Loss: 0.0065 | Train Acc: 100.00% | Val Loss: 0.6577 | Val Acc: 95.00%\n",
      "Batch loss: 0.038882311433553696\n",
      "Batch loss: 0.005715961568057537\n",
      "Batch loss: 0.0017907883739098907\n",
      "Batch loss: 9.639428753871471e-05\n",
      "Batch loss: 0.010854469612240791\n",
      "Epoch 81 | Train Loss: 0.0115 | Train Acc: 99.38% | Val Loss: 0.9174 | Val Acc: 87.50%\n",
      "Batch loss: 0.008790278807282448\n",
      "Batch loss: 0.005838549230247736\n",
      "Batch loss: 0.0001417285529896617\n",
      "Batch loss: 0.00019442243501544\n",
      "Batch loss: 0.027034509927034378\n",
      "Epoch 82 | Train Loss: 0.0084 | Train Acc: 100.00% | Val Loss: 0.9409 | Val Acc: 90.00%\n",
      "Batch loss: 0.0026534968055784702\n",
      "Batch loss: 0.0015944427577778697\n",
      "Batch loss: 0.005293992348015308\n",
      "Batch loss: 0.002023492008447647\n",
      "Batch loss: 0.002237471751868725\n",
      "Epoch 83 | Train Loss: 0.0028 | Train Acc: 100.00% | Val Loss: 0.9268 | Val Acc: 92.50%\n",
      "Batch loss: 0.0001834497379604727\n",
      "Batch loss: 0.004419912584125996\n",
      "Batch loss: 0.0013099313946440816\n",
      "Batch loss: 0.011929664760828018\n",
      "Batch loss: 0.001768035115674138\n",
      "Epoch 84 | Train Loss: 0.0039 | Train Acc: 100.00% | Val Loss: 0.8947 | Val Acc: 92.50%\n",
      "Batch loss: 0.00097212556283921\n",
      "Batch loss: 0.005207219626754522\n",
      "Batch loss: 0.01864871382713318\n",
      "Batch loss: 0.0003873338573612273\n",
      "Batch loss: 0.001753040705807507\n",
      "Epoch 85 | Train Loss: 0.0054 | Train Acc: 100.00% | Val Loss: 0.7806 | Val Acc: 95.00%\n",
      "Batch loss: 7.299012213479728e-05\n",
      "Batch loss: 0.03917009010910988\n",
      "Batch loss: 0.0021435434464365244\n",
      "Batch loss: 0.001318244612775743\n",
      "Batch loss: 2.8656584618147463e-05\n",
      "Epoch 86 | Train Loss: 0.0085 | Train Acc: 99.38% | Val Loss: 0.6918 | Val Acc: 92.50%\n",
      "Batch loss: 0.00010631637996993959\n",
      "Batch loss: 0.004688040353357792\n",
      "Batch loss: 0.023842010647058487\n",
      "Batch loss: 0.0005399987567216158\n",
      "Batch loss: 0.014014765620231628\n",
      "Epoch 87 | Train Loss: 0.0086 | Train Acc: 99.38% | Val Loss: 0.6185 | Val Acc: 95.00%\n",
      "Batch loss: 0.00013320037396624684\n",
      "Batch loss: 0.0007046982645988464\n",
      "Batch loss: 0.0005035044741816819\n",
      "Batch loss: 0.0003227935521863401\n",
      "Batch loss: 0.0008446969441138208\n",
      "Epoch 88 | Train Loss: 0.0005 | Train Acc: 100.00% | Val Loss: 0.6313 | Val Acc: 95.00%\n",
      "Batch loss: 0.0003903828328475356\n",
      "Batch loss: 0.0008232258842326701\n",
      "Batch loss: 0.0018773890333250165\n",
      "Batch loss: 0.012647243216633797\n",
      "Batch loss: 0.007432250771671534\n",
      "Epoch 89 | Train Loss: 0.0046 | Train Acc: 100.00% | Val Loss: 0.6574 | Val Acc: 95.00%\n",
      "Batch loss: 0.0010396643774583936\n",
      "Batch loss: 0.00012720887025352567\n",
      "Batch loss: 0.0515572652220726\n",
      "Batch loss: 0.0009865573374554515\n",
      "Batch loss: 0.0011032611364498734\n",
      "Epoch 90 | Train Loss: 0.0110 | Train Acc: 99.38% | Val Loss: 0.2956 | Val Acc: 92.50%\n",
      "Batch loss: 0.000445802987087518\n",
      "Batch loss: 0.004954436793923378\n",
      "Batch loss: 0.0009783870773389935\n",
      "Batch loss: 0.004011969547718763\n",
      "Batch loss: 0.0014774088049307466\n",
      "Epoch 91 | Train Loss: 0.0024 | Train Acc: 100.00% | Val Loss: 0.3280 | Val Acc: 90.00%\n",
      "Batch loss: 0.00043318135431036353\n",
      "Batch loss: 0.04082749783992767\n",
      "Batch loss: 0.04174954444169998\n",
      "Batch loss: 0.03074180707335472\n",
      "Batch loss: 0.0027744059916585684\n",
      "Epoch 92 | Train Loss: 0.0233 | Train Acc: 98.75% | Val Loss: 0.4938 | Val Acc: 92.50%\n",
      "Batch loss: 0.0004054731107316911\n",
      "Batch loss: 0.00247928686439991\n",
      "Batch loss: 0.022032083943486214\n",
      "Batch loss: 0.008425458334386349\n",
      "Batch loss: 0.009898684918880463\n",
      "Epoch 93 | Train Loss: 0.0086 | Train Acc: 100.00% | Val Loss: 1.3671 | Val Acc: 77.50%\n",
      "Batch loss: 0.038575444370508194\n",
      "Batch loss: 0.023807795718312263\n",
      "Batch loss: 0.023764749988913536\n",
      "Batch loss: 0.000613959738984704\n",
      "Batch loss: 0.10667088627815247\n",
      "Epoch 94 | Train Loss: 0.0387 | Train Acc: 98.75% | Val Loss: 1.0512 | Val Acc: 85.00%\n",
      "Batch loss: 0.00663653714582324\n",
      "Batch loss: 0.023616379126906395\n",
      "Batch loss: 0.001811928115785122\n",
      "Batch loss: 0.013718273490667343\n",
      "Batch loss: 0.012560964561998844\n",
      "Epoch 95 | Train Loss: 0.0117 | Train Acc: 100.00% | Val Loss: 0.3290 | Val Acc: 92.50%\n",
      "Batch loss: 0.010408265516161919\n",
      "Batch loss: 0.03109034337103367\n",
      "Batch loss: 0.002205152530223131\n",
      "Batch loss: 0.015025045722723007\n",
      "Batch loss: 0.02138855867087841\n",
      "Epoch 96 | Train Loss: 0.0160 | Train Acc: 99.38% | Val Loss: 0.3832 | Val Acc: 92.50%\n",
      "Batch loss: 0.05010047182440758\n",
      "Batch loss: 0.0024130307137966156\n",
      "Batch loss: 0.06098359450697899\n",
      "Batch loss: 0.023244405165314674\n",
      "Batch loss: 0.01355660893023014\n",
      "Epoch 97 | Train Loss: 0.0301 | Train Acc: 98.12% | Val Loss: 0.4066 | Val Acc: 92.50%\n",
      "Batch loss: 0.02728532999753952\n",
      "Batch loss: 0.02386428788304329\n",
      "Batch loss: 0.0003063389449380338\n",
      "Batch loss: 0.0022477032616734505\n",
      "Batch loss: 0.0016449897084385157\n",
      "Epoch 98 | Train Loss: 0.0111 | Train Acc: 100.00% | Val Loss: 0.6993 | Val Acc: 95.00%\n",
      "Batch loss: 0.0449613519012928\n",
      "Batch loss: 0.0032739704474806786\n",
      "Batch loss: 0.0012444754829630256\n",
      "Batch loss: 0.0005617261631414294\n",
      "Batch loss: 0.006178957410156727\n",
      "Epoch 99 | Train Loss: 0.0112 | Train Acc: 99.38% | Val Loss: 0.5840 | Val Acc: 97.50%\n",
      "Batch loss: 0.001640533795580268\n",
      "Batch loss: 0.019462324678897858\n",
      "Batch loss: 0.011594107374548912\n",
      "Batch loss: 0.0004277275875210762\n",
      "Batch loss: 0.009650811553001404\n",
      "Epoch 100 | Train Loss: 0.0086 | Train Acc: 100.00% | Val Loss: 0.6655 | Val Acc: 97.50%\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def train_model(epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            print(f\"Batch loss: {loss.item()}\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "# Assuming you already have a val_loader for your validation dataset\n",
    "\n",
    "train_model(epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19a75192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset split done! Train and val folders created at 'split_dat'\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/codespace/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 125MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.8098 | Train Acc: 49.38% | Val Loss: 0.6427 | Val Acc: 55.00%\n",
      "Epoch 2 | Train Loss: 0.6459 | Train Acc: 66.88% | Val Loss: 0.5559 | Val Acc: 62.50%\n",
      "Epoch 3 | Train Loss: 0.5841 | Train Acc: 72.50% | Val Loss: 0.5622 | Val Acc: 75.00%\n",
      "Epoch 4 | Train Loss: 0.4728 | Train Acc: 78.12% | Val Loss: 0.4818 | Val Acc: 85.00%\n",
      "Epoch 5 | Train Loss: 0.4298 | Train Acc: 84.38% | Val Loss: 0.4492 | Val Acc: 85.00%\n",
      "Epoch 6 | Train Loss: 0.3756 | Train Acc: 86.88% | Val Loss: 0.4484 | Val Acc: 87.50%\n",
      "Epoch 7 | Train Loss: 0.3404 | Train Acc: 88.12% | Val Loss: 0.4034 | Val Acc: 85.00%\n",
      "Epoch 8 | Train Loss: 0.3033 | Train Acc: 93.75% | Val Loss: 0.4014 | Val Acc: 87.50%\n",
      "Epoch 9 | Train Loss: 0.2892 | Train Acc: 93.12% | Val Loss: 0.4137 | Val Acc: 87.50%\n",
      "Epoch 10 | Train Loss: 0.2579 | Train Acc: 95.00% | Val Loss: 0.3797 | Val Acc: 90.00%\n",
      "\n",
      "🏆 Best Val Accuracy: 90.00%\n"
     ]
    }
   ],
   "source": [
    "# === PARAMETERS ===\n",
    "root_folder = 'aug_processed_data'  # Folder with 'healthy' and 'unhealthy' subfolders\n",
    "base_dir = 'split_dat'                   # New folder where train/val split will go\n",
    "train_ratio = 0.8                         # 80% train, 20% val split\n",
    "\n",
    "# === 1. Create train/val folders with same subfolder structure ===\n",
    "def create_train_val_split(root_folder, base_dir, train_ratio=0.8):\n",
    "    if os.path.exists(base_dir):\n",
    "        shutil.rmtree(base_dir)  # Clean previous split if exists\n",
    "    os.makedirs(base_dir)\n",
    "\n",
    "    train_dir = os.path.join(base_dir, 'train')\n",
    "    val_dir = os.path.join(base_dir, 'val')\n",
    "\n",
    "    os.makedirs(train_dir)\n",
    "    os.makedirs(val_dir)\n",
    "\n",
    "    classes = [d for d in os.listdir(root_folder) if os.path.isdir(os.path.join(root_folder, d))]\n",
    "\n",
    "    for cls in classes:\n",
    "        os.makedirs(os.path.join(train_dir, cls))\n",
    "        os.makedirs(os.path.join(val_dir, cls))\n",
    "\n",
    "        images = os.listdir(os.path.join(root_folder, cls))\n",
    "        random.shuffle(images)\n",
    "\n",
    "        train_count = int(len(images) * train_ratio)\n",
    "        train_imgs = images[:train_count]\n",
    "        val_imgs = images[train_count:]\n",
    "\n",
    "        for img_name in train_imgs:\n",
    "            src = os.path.join(root_folder, cls, img_name)\n",
    "            dst = os.path.join(train_dir, cls, img_name)\n",
    "            shutil.copy(src, dst)\n",
    "\n",
    "        for img_name in val_imgs:\n",
    "            src = os.path.join(root_folder, cls, img_name)\n",
    "            dst = os.path.join(val_dir, cls, img_name)\n",
    "            shutil.copy(src, dst)\n",
    "\n",
    "    print(f\"✅ Dataset split done! Train and val folders created at '{base_dir}'\")\n",
    "\n",
    "# === Run the split function ===\n",
    "create_train_val_split(root_folder, base_dir, train_ratio)\n",
    "\n",
    "# === 2. Setup Transfer Learning with the split folders ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(os.path.join(base_dir, 'train'), transform=transform)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(base_dir, 'val'), transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # Freeze backbone\n",
    "\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, len(train_dataset.classes))  # Number of classes\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "# === Training loop ===\n",
    "def train_model(epochs=10):\n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_correct, val_total, val_loss = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {avg_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    torch.save(model.state_dict(), \"plant_model.pth\")\n",
    "    print(f\"\\n🏆 Best Val Accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "# === Run training ===\n",
    "train_model(epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa46f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
