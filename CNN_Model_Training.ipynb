{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b25eb04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaae90d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder('aug_processed_data', transform=transform)\n",
    "\n",
    "# Train/val split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39fbb19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 32 * 32, 64)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))   # 64x64\n",
    "        x = self.pool(F.relu(self.conv2(x)))   # 32x32\n",
    "        x = x.view(-1, 32 * 32 * 32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da11de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Approch 2\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Dropout with 50% probability\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * 32 * 32, 64)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))   # 128->64\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))   # 64->32\n",
    "\n",
    "        x = x.view(-1, 32 * 32 * 32)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))            # Dropout before fc2\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "946f0ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # weight_decay added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "043a7e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7063717246055603\n",
      "Batch loss: 8.024457931518555\n",
      "Batch loss: 6.98690938949585\n",
      "Batch loss: 3.708674669265747\n",
      "Batch loss: 1.4885411262512207\n",
      "Epoch 1 | Train Loss: 4.1830 | Train Acc: 55.00% | Val Loss: 1.1323 | Val Acc: 45.00%\n",
      "Batch loss: 4.182121276855469\n",
      "Batch loss: 3.6933205127716064\n",
      "Batch loss: 5.5826802253723145\n",
      "Batch loss: 1.6650841236114502\n",
      "Batch loss: 4.137962818145752\n",
      "Epoch 2 | Train Loss: 3.8522 | Train Acc: 66.25% | Val Loss: 0.4519 | Val Acc: 70.00%\n",
      "Batch loss: 2.2397427558898926\n",
      "Batch loss: 3.4800338745117188\n",
      "Batch loss: 0.7653477787971497\n",
      "Batch loss: 1.0923881530761719\n",
      "Batch loss: 0.4507090151309967\n",
      "Epoch 3 | Train Loss: 1.6056 | Train Acc: 78.12% | Val Loss: 0.7183 | Val Acc: 75.00%\n",
      "Batch loss: 1.3926022052764893\n",
      "Batch loss: 0.33079490065574646\n",
      "Batch loss: 0.6197651028633118\n",
      "Batch loss: 0.4237731993198395\n",
      "Batch loss: 0.3708096444606781\n",
      "Epoch 4 | Train Loss: 0.6275 | Train Acc: 83.12% | Val Loss: 0.4064 | Val Acc: 80.00%\n",
      "Batch loss: 0.20485670864582062\n",
      "Batch loss: 0.1939542293548584\n",
      "Batch loss: 0.48528337478637695\n",
      "Batch loss: 0.1692054122686386\n",
      "Batch loss: 0.38766467571258545\n",
      "Epoch 5 | Train Loss: 0.2882 | Train Acc: 85.62% | Val Loss: 0.3666 | Val Acc: 92.50%\n",
      "Batch loss: 0.29735440015792847\n",
      "Batch loss: 0.3392970860004425\n",
      "Batch loss: 0.23727300763130188\n",
      "Batch loss: 0.2676028907299042\n",
      "Batch loss: 0.2891322374343872\n",
      "Epoch 6 | Train Loss: 0.2861 | Train Acc: 85.00% | Val Loss: 0.3979 | Val Acc: 80.00%\n",
      "Batch loss: 0.24557378888130188\n",
      "Batch loss: 0.26384466886520386\n",
      "Batch loss: 0.28456911444664\n",
      "Batch loss: 0.18811412155628204\n",
      "Batch loss: 0.3242473006248474\n",
      "Epoch 7 | Train Loss: 0.2613 | Train Acc: 90.00% | Val Loss: 0.3554 | Val Acc: 85.00%\n",
      "Batch loss: 0.20224513113498688\n",
      "Batch loss: 0.16260264813899994\n",
      "Batch loss: 0.21031999588012695\n",
      "Batch loss: 0.3791515529155731\n",
      "Batch loss: 0.16760782897472382\n",
      "Epoch 8 | Train Loss: 0.2244 | Train Acc: 86.88% | Val Loss: 0.3517 | Val Acc: 87.50%\n",
      "Batch loss: 0.1602431833744049\n",
      "Batch loss: 0.18330006301403046\n",
      "Batch loss: 0.20988406240940094\n",
      "Batch loss: 0.3877643048763275\n",
      "Batch loss: 0.14434583485126495\n",
      "Epoch 9 | Train Loss: 0.2171 | Train Acc: 90.62% | Val Loss: 0.4288 | Val Acc: 85.00%\n",
      "Batch loss: 0.18582823872566223\n",
      "Batch loss: 0.17780764400959015\n",
      "Batch loss: 0.1734800934791565\n",
      "Batch loss: 0.11029903590679169\n",
      "Batch loss: 0.1462353616952896\n",
      "Epoch 10 | Train Loss: 0.1587 | Train Acc: 93.75% | Val Loss: 0.4775 | Val Acc: 85.00%\n",
      "Batch loss: 0.18507856130599976\n",
      "Batch loss: 0.1432410329580307\n",
      "Batch loss: 0.10491013526916504\n",
      "Batch loss: 0.15953239798545837\n",
      "Batch loss: 0.1376160979270935\n",
      "Epoch 11 | Train Loss: 0.1461 | Train Acc: 93.75% | Val Loss: 0.4202 | Val Acc: 87.50%\n",
      "Batch loss: 0.11855099350214005\n",
      "Batch loss: 0.12835951149463654\n",
      "Batch loss: 0.1929941028356552\n",
      "Batch loss: 0.11048863083124161\n",
      "Batch loss: 0.14165566861629486\n",
      "Epoch 12 | Train Loss: 0.1384 | Train Acc: 93.12% | Val Loss: 0.4939 | Val Acc: 90.00%\n",
      "Batch loss: 0.10675666481256485\n",
      "Batch loss: 0.16594375669956207\n",
      "Batch loss: 0.10516597330570221\n",
      "Batch loss: 0.08296749740839005\n",
      "Batch loss: 0.10913475602865219\n",
      "Epoch 13 | Train Loss: 0.1140 | Train Acc: 94.38% | Val Loss: 0.5921 | Val Acc: 90.00%\n",
      "Batch loss: 0.0665416270494461\n",
      "Batch loss: 0.1146007552742958\n",
      "Batch loss: 0.07002981752157211\n",
      "Batch loss: 0.2439221739768982\n",
      "Batch loss: 0.08200289309024811\n",
      "Epoch 14 | Train Loss: 0.1154 | Train Acc: 95.62% | Val Loss: 0.5678 | Val Acc: 85.00%\n",
      "Batch loss: 0.059031516313552856\n",
      "Batch loss: 0.12418802827596664\n",
      "Batch loss: 0.10024935007095337\n",
      "Batch loss: 0.053733713924884796\n",
      "Batch loss: 0.18285737931728363\n",
      "Epoch 15 | Train Loss: 0.1040 | Train Acc: 95.62% | Val Loss: 0.3812 | Val Acc: 90.00%\n",
      "Batch loss: 0.0802401453256607\n",
      "Batch loss: 0.07682241499423981\n",
      "Batch loss: 0.11858228594064713\n",
      "Batch loss: 0.14748486876487732\n",
      "Batch loss: 0.13158540427684784\n",
      "Epoch 16 | Train Loss: 0.1109 | Train Acc: 91.88% | Val Loss: 0.5450 | Val Acc: 90.00%\n",
      "Batch loss: 0.07979419827461243\n",
      "Batch loss: 0.07153892517089844\n",
      "Batch loss: 0.049752458930015564\n",
      "Batch loss: 0.14624589681625366\n",
      "Batch loss: 0.08574674278497696\n",
      "Epoch 17 | Train Loss: 0.0866 | Train Acc: 95.62% | Val Loss: 0.5675 | Val Acc: 90.00%\n",
      "Batch loss: 0.10411491245031357\n",
      "Batch loss: 0.043319642543792725\n",
      "Batch loss: 0.1606907993555069\n",
      "Batch loss: 0.07710874825716019\n",
      "Batch loss: 0.15601561963558197\n",
      "Epoch 18 | Train Loss: 0.1082 | Train Acc: 95.62% | Val Loss: 0.6074 | Val Acc: 90.00%\n",
      "Batch loss: 0.06718597561120987\n",
      "Batch loss: 0.06645620614290237\n",
      "Batch loss: 0.0731128677725792\n",
      "Batch loss: 0.08486981689929962\n",
      "Batch loss: 0.07439184188842773\n",
      "Epoch 19 | Train Loss: 0.0732 | Train Acc: 97.50% | Val Loss: 0.5624 | Val Acc: 90.00%\n",
      "Batch loss: 0.10670951753854752\n",
      "Batch loss: 0.12109175324440002\n",
      "Batch loss: 0.07625407725572586\n",
      "Batch loss: 0.05395324528217316\n",
      "Batch loss: 0.09058499336242676\n",
      "Epoch 20 | Train Loss: 0.0897 | Train Acc: 95.00% | Val Loss: 0.6743 | Val Acc: 87.50%\n",
      "Batch loss: 0.09100473672151566\n",
      "Batch loss: 0.05613437294960022\n",
      "Batch loss: 0.010572515428066254\n",
      "Batch loss: 0.07580187916755676\n",
      "Batch loss: 0.07060141861438751\n",
      "Epoch 21 | Train Loss: 0.0608 | Train Acc: 97.50% | Val Loss: 0.7293 | Val Acc: 87.50%\n",
      "Batch loss: 0.0787920355796814\n",
      "Batch loss: 0.054231710731983185\n",
      "Batch loss: 0.08782733976840973\n",
      "Batch loss: 0.015162565745413303\n",
      "Batch loss: 0.037650953978300095\n",
      "Epoch 22 | Train Loss: 0.0547 | Train Acc: 96.88% | Val Loss: 0.7497 | Val Acc: 87.50%\n",
      "Batch loss: 0.11567331850528717\n",
      "Batch loss: 0.07273441553115845\n",
      "Batch loss: 0.05878765881061554\n",
      "Batch loss: 0.1452292501926422\n",
      "Batch loss: 0.09018565714359283\n",
      "Epoch 23 | Train Loss: 0.0965 | Train Acc: 93.75% | Val Loss: 0.8462 | Val Acc: 87.50%\n",
      "Batch loss: 0.013270451687276363\n",
      "Batch loss: 0.11019421368837357\n",
      "Batch loss: 0.07246916741132736\n",
      "Batch loss: 0.049792349338531494\n",
      "Batch loss: 0.008956419304013252\n",
      "Epoch 24 | Train Loss: 0.0509 | Train Acc: 98.12% | Val Loss: 0.8593 | Val Acc: 87.50%\n",
      "Batch loss: 0.07042898237705231\n",
      "Batch loss: 0.10730180144309998\n",
      "Batch loss: 0.026958821341395378\n",
      "Batch loss: 0.012258278205990791\n",
      "Batch loss: 0.03702516108751297\n",
      "Epoch 25 | Train Loss: 0.0508 | Train Acc: 98.75% | Val Loss: 0.8499 | Val Acc: 92.50%\n",
      "Batch loss: 0.041626591235399246\n",
      "Batch loss: 0.06635037064552307\n",
      "Batch loss: 0.0809311643242836\n",
      "Batch loss: 0.03442428261041641\n",
      "Batch loss: 0.020411416888237\n",
      "Epoch 26 | Train Loss: 0.0487 | Train Acc: 97.50% | Val Loss: 0.7837 | Val Acc: 92.50%\n",
      "Batch loss: 0.0784478709101677\n",
      "Batch loss: 0.05425169691443443\n",
      "Batch loss: 0.021003680303692818\n",
      "Batch loss: 0.048359453678131104\n",
      "Batch loss: 0.15921930968761444\n",
      "Epoch 27 | Train Loss: 0.0723 | Train Acc: 96.25% | Val Loss: 0.8828 | Val Acc: 87.50%\n",
      "Batch loss: 0.03730356693267822\n",
      "Batch loss: 0.005098733585327864\n",
      "Batch loss: 0.061454884707927704\n",
      "Batch loss: 0.049064721912145615\n",
      "Batch loss: 0.006708796136081219\n",
      "Epoch 28 | Train Loss: 0.0319 | Train Acc: 98.12% | Val Loss: 0.6471 | Val Acc: 92.50%\n",
      "Batch loss: 0.02720111981034279\n",
      "Batch loss: 0.047537509351968765\n",
      "Batch loss: 0.004163756966590881\n",
      "Batch loss: 0.017776962369680405\n",
      "Batch loss: 0.0038874936290085316\n",
      "Epoch 29 | Train Loss: 0.0201 | Train Acc: 99.38% | Val Loss: 0.5250 | Val Acc: 90.00%\n",
      "Batch loss: 0.022184142842888832\n",
      "Batch loss: 0.03137904405593872\n",
      "Batch loss: 0.16551148891448975\n",
      "Batch loss: 0.05321245267987251\n",
      "Batch loss: 0.056724827736616135\n",
      "Epoch 30 | Train Loss: 0.0658 | Train Acc: 97.50% | Val Loss: 0.5992 | Val Acc: 92.50%\n",
      "Batch loss: 0.03267611190676689\n",
      "Batch loss: 0.011481923051178455\n",
      "Batch loss: 0.02810967154800892\n",
      "Batch loss: 0.05899200215935707\n",
      "Batch loss: 0.07268527150154114\n",
      "Epoch 31 | Train Loss: 0.0408 | Train Acc: 97.50% | Val Loss: 0.6909 | Val Acc: 92.50%\n",
      "Batch loss: 0.06949811428785324\n",
      "Batch loss: 0.019794005900621414\n",
      "Batch loss: 0.012495895847678185\n",
      "Batch loss: 0.03866725042462349\n",
      "Batch loss: 0.011114580556750298\n",
      "Epoch 32 | Train Loss: 0.0303 | Train Acc: 98.75% | Val Loss: 0.7072 | Val Acc: 92.50%\n",
      "Batch loss: 0.036067187786102295\n",
      "Batch loss: 0.032843757420778275\n",
      "Batch loss: 0.05533328652381897\n",
      "Batch loss: 0.04905958101153374\n",
      "Batch loss: 0.06359680742025375\n",
      "Epoch 33 | Train Loss: 0.0474 | Train Acc: 97.50% | Val Loss: 0.8013 | Val Acc: 92.50%\n",
      "Batch loss: 0.02906506322324276\n",
      "Batch loss: 0.09911726415157318\n",
      "Batch loss: 0.013965865597128868\n",
      "Batch loss: 0.02974584326148033\n",
      "Batch loss: 0.05103589966893196\n",
      "Epoch 34 | Train Loss: 0.0446 | Train Acc: 99.38% | Val Loss: 0.9307 | Val Acc: 90.00%\n",
      "Batch loss: 0.01598401367664337\n",
      "Batch loss: 0.03053591027855873\n",
      "Batch loss: 0.03324500843882561\n",
      "Batch loss: 0.06894097477197647\n",
      "Batch loss: 0.006933706812560558\n",
      "Epoch 35 | Train Loss: 0.0311 | Train Acc: 98.75% | Val Loss: 0.8544 | Val Acc: 92.50%\n",
      "Batch loss: 0.018720263615250587\n",
      "Batch loss: 0.0165190938860178\n",
      "Batch loss: 0.010334658436477184\n",
      "Batch loss: 0.022941606119275093\n",
      "Batch loss: 0.08037693798542023\n",
      "Epoch 36 | Train Loss: 0.0298 | Train Acc: 98.75% | Val Loss: 0.7427 | Val Acc: 92.50%\n",
      "Batch loss: 0.06332138925790787\n",
      "Batch loss: 0.06042301654815674\n",
      "Batch loss: 0.002215440385043621\n",
      "Batch loss: 0.05007965490221977\n",
      "Batch loss: 0.07060221582651138\n",
      "Epoch 37 | Train Loss: 0.0493 | Train Acc: 96.88% | Val Loss: 0.8303 | Val Acc: 92.50%\n",
      "Batch loss: 0.05736297741532326\n",
      "Batch loss: 0.01638941466808319\n",
      "Batch loss: 0.020005322992801666\n",
      "Batch loss: 0.03704238310456276\n",
      "Batch loss: 0.13516725599765778\n",
      "Epoch 38 | Train Loss: 0.0532 | Train Acc: 98.12% | Val Loss: 0.8134 | Val Acc: 92.50%\n",
      "Batch loss: 0.038402799516916275\n",
      "Batch loss: 0.007960853166878223\n",
      "Batch loss: 0.03189907968044281\n",
      "Batch loss: 0.018540943041443825\n",
      "Batch loss: 0.10329139232635498\n",
      "Epoch 39 | Train Loss: 0.0400 | Train Acc: 98.12% | Val Loss: 1.1749 | Val Acc: 82.50%\n",
      "Batch loss: 0.11473242938518524\n",
      "Batch loss: 0.025470592081546783\n",
      "Batch loss: 0.03976074606180191\n",
      "Batch loss: 0.04573311284184456\n",
      "Batch loss: 0.03931904211640358\n",
      "Epoch 40 | Train Loss: 0.0530 | Train Acc: 96.88% | Val Loss: 0.4438 | Val Acc: 90.00%\n",
      "Batch loss: 0.052810367196798325\n",
      "Batch loss: 0.08028782159090042\n",
      "Batch loss: 0.040630292147397995\n",
      "Batch loss: 0.04477653279900551\n",
      "Batch loss: 0.046196822077035904\n",
      "Epoch 41 | Train Loss: 0.0529 | Train Acc: 98.12% | Val Loss: 0.4527 | Val Acc: 92.50%\n",
      "Batch loss: 0.06073839217424393\n",
      "Batch loss: 0.02832997962832451\n",
      "Batch loss: 0.03493586555123329\n",
      "Batch loss: 0.014914141967892647\n",
      "Batch loss: 0.016165394335985184\n",
      "Epoch 42 | Train Loss: 0.0310 | Train Acc: 99.38% | Val Loss: 0.6885 | Val Acc: 90.00%\n",
      "Batch loss: 0.046174027025699615\n",
      "Batch loss: 0.01604294590651989\n",
      "Batch loss: 0.0326472744345665\n",
      "Batch loss: 0.030198829248547554\n",
      "Batch loss: 0.03768754377961159\n",
      "Epoch 43 | Train Loss: 0.0326 | Train Acc: 98.75% | Val Loss: 1.0633 | Val Acc: 82.50%\n",
      "Batch loss: 0.004170005675405264\n",
      "Batch loss: 0.1066342294216156\n",
      "Batch loss: 0.04963991418480873\n",
      "Batch loss: 0.004364040680229664\n",
      "Batch loss: 0.011536176316440105\n",
      "Epoch 44 | Train Loss: 0.0353 | Train Acc: 98.12% | Val Loss: 0.9043 | Val Acc: 92.50%\n",
      "Batch loss: 0.0005797009798698127\n",
      "Batch loss: 0.006710425019264221\n",
      "Batch loss: 0.02018473483622074\n",
      "Batch loss: 0.0022418235894292593\n",
      "Batch loss: 0.05094415694475174\n",
      "Epoch 45 | Train Loss: 0.0161 | Train Acc: 99.38% | Val Loss: 0.8318 | Val Acc: 92.50%\n",
      "Batch loss: 0.010124816559255123\n",
      "Batch loss: 0.02334662899374962\n",
      "Batch loss: 0.09660420566797256\n",
      "Batch loss: 0.007431713864207268\n",
      "Batch loss: 0.19525805115699768\n",
      "Epoch 46 | Train Loss: 0.0666 | Train Acc: 97.50% | Val Loss: 0.9053 | Val Acc: 92.50%\n",
      "Batch loss: 0.0009351164335384965\n",
      "Batch loss: 0.0004330566735006869\n",
      "Batch loss: 0.044346630573272705\n",
      "Batch loss: 0.029514577239751816\n",
      "Batch loss: 0.0845770537853241\n",
      "Epoch 47 | Train Loss: 0.0320 | Train Acc: 99.38% | Val Loss: 1.5845 | Val Acc: 82.50%\n",
      "Batch loss: 0.012291583232581615\n",
      "Batch loss: 0.09529586136341095\n",
      "Batch loss: 0.01803174987435341\n",
      "Batch loss: 0.03943892940878868\n",
      "Batch loss: 0.02728036418557167\n",
      "Epoch 48 | Train Loss: 0.0385 | Train Acc: 98.75% | Val Loss: 0.9887 | Val Acc: 90.00%\n",
      "Batch loss: 0.07402648031711578\n",
      "Batch loss: 0.02283003181219101\n",
      "Batch loss: 0.033671896904706955\n",
      "Batch loss: 0.029218891635537148\n",
      "Batch loss: 0.03370947763323784\n",
      "Epoch 49 | Train Loss: 0.0387 | Train Acc: 98.75% | Val Loss: 0.5953 | Val Acc: 92.50%\n",
      "Batch loss: 0.006209195591509342\n",
      "Batch loss: 0.07530029863119125\n",
      "Batch loss: 0.026519929990172386\n",
      "Batch loss: 0.05464803799986839\n",
      "Batch loss: 0.07772669196128845\n",
      "Epoch 50 | Train Loss: 0.0481 | Train Acc: 97.50% | Val Loss: 0.5914 | Val Acc: 92.50%\n",
      "Batch loss: 0.049089718610048294\n",
      "Batch loss: 0.01744740828871727\n",
      "Batch loss: 0.006190345622599125\n",
      "Batch loss: 0.02806561253964901\n",
      "Batch loss: 0.01718916930258274\n",
      "Epoch 51 | Train Loss: 0.0236 | Train Acc: 99.38% | Val Loss: 0.8682 | Val Acc: 92.50%\n",
      "Batch loss: 0.029923753812909126\n",
      "Batch loss: 0.011861633509397507\n",
      "Batch loss: 0.040754519402980804\n",
      "Batch loss: 0.02294963225722313\n",
      "Batch loss: 0.02660711482167244\n",
      "Epoch 52 | Train Loss: 0.0264 | Train Acc: 98.75% | Val Loss: 1.0484 | Val Acc: 92.50%\n",
      "Batch loss: 0.023971309885382652\n",
      "Batch loss: 0.026300249621272087\n",
      "Batch loss: 0.028177347034215927\n",
      "Batch loss: 0.01641768030822277\n",
      "Batch loss: 0.027031157165765762\n",
      "Epoch 53 | Train Loss: 0.0244 | Train Acc: 98.75% | Val Loss: 1.0614 | Val Acc: 92.50%\n",
      "Batch loss: 0.03686663135886192\n",
      "Batch loss: 0.025917327031493187\n",
      "Batch loss: 0.025870852172374725\n",
      "Batch loss: 0.09202917665243149\n",
      "Batch loss: 0.033266935497522354\n",
      "Epoch 54 | Train Loss: 0.0428 | Train Acc: 97.50% | Val Loss: 0.9850 | Val Acc: 92.50%\n",
      "Batch loss: 0.0023632377851754427\n",
      "Batch loss: 0.026103591546416283\n",
      "Batch loss: 0.005721112247556448\n",
      "Batch loss: 0.009897888638079166\n",
      "Batch loss: 0.022020747885107994\n",
      "Epoch 55 | Train Loss: 0.0132 | Train Acc: 100.00% | Val Loss: 0.8461 | Val Acc: 92.50%\n",
      "Batch loss: 0.04873211309313774\n",
      "Batch loss: 0.005461311899125576\n",
      "Batch loss: 0.0026331807021051645\n",
      "Batch loss: 0.001206778921186924\n",
      "Batch loss: 0.006375287659466267\n",
      "Epoch 56 | Train Loss: 0.0129 | Train Acc: 99.38% | Val Loss: 0.9169 | Val Acc: 92.50%\n",
      "Batch loss: 0.0016810346860438585\n",
      "Batch loss: 0.0033254087902605534\n",
      "Batch loss: 0.00031318352557718754\n",
      "Batch loss: 0.0017064283601939678\n",
      "Batch loss: 0.02229832485318184\n",
      "Epoch 57 | Train Loss: 0.0059 | Train Acc: 100.00% | Val Loss: 0.9832 | Val Acc: 92.50%\n",
      "Batch loss: 0.037070587277412415\n",
      "Batch loss: 0.023931238800287247\n",
      "Batch loss: 0.09652625769376755\n",
      "Batch loss: 0.08289363235235214\n",
      "Batch loss: 0.004343828186392784\n",
      "Epoch 58 | Train Loss: 0.0490 | Train Acc: 97.50% | Val Loss: 1.1885 | Val Acc: 90.00%\n",
      "Batch loss: 0.04854883998632431\n",
      "Batch loss: 0.02357199601829052\n",
      "Batch loss: 0.04631267488002777\n",
      "Batch loss: 0.0025138442870229483\n",
      "Batch loss: 0.0028666432481259108\n",
      "Epoch 59 | Train Loss: 0.0248 | Train Acc: 98.75% | Val Loss: 1.1999 | Val Acc: 92.50%\n",
      "Batch loss: 0.01607213169336319\n",
      "Batch loss: 0.002224076073616743\n",
      "Batch loss: 0.04385728761553764\n",
      "Batch loss: 0.02506656013429165\n",
      "Batch loss: 0.013481667265295982\n",
      "Epoch 60 | Train Loss: 0.0201 | Train Acc: 100.00% | Val Loss: 0.9946 | Val Acc: 92.50%\n",
      "Batch loss: 0.026669872924685478\n",
      "Batch loss: 0.02810937538743019\n",
      "Batch loss: 0.007400658912956715\n",
      "Batch loss: 0.007947757840156555\n",
      "Batch loss: 0.021154742687940598\n",
      "Epoch 61 | Train Loss: 0.0183 | Train Acc: 98.75% | Val Loss: 0.8159 | Val Acc: 90.00%\n",
      "Batch loss: 0.02467125467956066\n",
      "Batch loss: 0.0027340445667505264\n",
      "Batch loss: 0.021308396011590958\n",
      "Batch loss: 0.029990609735250473\n",
      "Batch loss: 0.014043183997273445\n",
      "Epoch 62 | Train Loss: 0.0185 | Train Acc: 98.75% | Val Loss: 0.8327 | Val Acc: 90.00%\n",
      "Batch loss: 0.01551145687699318\n",
      "Batch loss: 0.18818244338035583\n",
      "Batch loss: 0.08367715030908585\n",
      "Batch loss: 0.008009536191821098\n",
      "Batch loss: 0.0009278592769987881\n",
      "Epoch 63 | Train Loss: 0.0593 | Train Acc: 98.12% | Val Loss: 1.1128 | Val Acc: 90.00%\n",
      "Batch loss: 0.024265550076961517\n",
      "Batch loss: 0.00541356997564435\n",
      "Batch loss: 0.04655596986413002\n",
      "Batch loss: 0.00040723226265981793\n",
      "Batch loss: 0.047938331961631775\n",
      "Epoch 64 | Train Loss: 0.0249 | Train Acc: 98.75% | Val Loss: 1.2427 | Val Acc: 87.50%\n",
      "Batch loss: 0.041712306439876556\n",
      "Batch loss: 0.019152715802192688\n",
      "Batch loss: 0.000862521177623421\n",
      "Batch loss: 0.025846406817436218\n",
      "Batch loss: 0.09949927031993866\n",
      "Epoch 65 | Train Loss: 0.0374 | Train Acc: 98.75% | Val Loss: 1.2529 | Val Acc: 87.50%\n",
      "Batch loss: 0.008084611035883427\n",
      "Batch loss: 0.027337882667779922\n",
      "Batch loss: 0.05860058218240738\n",
      "Batch loss: 0.008255615830421448\n",
      "Batch loss: 0.016881246119737625\n",
      "Epoch 66 | Train Loss: 0.0238 | Train Acc: 98.12% | Val Loss: 1.4520 | Val Acc: 82.50%\n",
      "Batch loss: 0.03306426852941513\n",
      "Batch loss: 0.026835449039936066\n",
      "Batch loss: 0.006593492813408375\n",
      "Batch loss: 0.0019743486773222685\n",
      "Batch loss: 0.029181161895394325\n",
      "Epoch 67 | Train Loss: 0.0195 | Train Acc: 99.38% | Val Loss: 1.3347 | Val Acc: 87.50%\n",
      "Batch loss: 0.01818334311246872\n",
      "Batch loss: 0.0051528895273804665\n",
      "Batch loss: 0.03050820343196392\n",
      "Batch loss: 0.029599810019135475\n",
      "Batch loss: 0.002415229333564639\n",
      "Epoch 68 | Train Loss: 0.0172 | Train Acc: 99.38% | Val Loss: 1.2230 | Val Acc: 90.00%\n",
      "Batch loss: 0.05342642217874527\n",
      "Batch loss: 0.012032200582325459\n",
      "Batch loss: 0.0030112138483673334\n",
      "Batch loss: 0.015381463803350925\n",
      "Batch loss: 0.051864806562662125\n",
      "Epoch 69 | Train Loss: 0.0271 | Train Acc: 98.75% | Val Loss: 1.1386 | Val Acc: 92.50%\n",
      "Batch loss: 0.06285148113965988\n",
      "Batch loss: 0.0008958190446719527\n",
      "Batch loss: 0.012907120399177074\n",
      "Batch loss: 0.010517874732613564\n",
      "Batch loss: 0.0008354899473488331\n",
      "Epoch 70 | Train Loss: 0.0176 | Train Acc: 100.00% | Val Loss: 1.3567 | Val Acc: 90.00%\n",
      "Batch loss: 0.06759856641292572\n",
      "Batch loss: 0.02467486634850502\n",
      "Batch loss: 0.02680617943406105\n",
      "Batch loss: 0.024155981838703156\n",
      "Batch loss: 0.0004198296519462019\n",
      "Epoch 71 | Train Loss: 0.0287 | Train Acc: 96.88% | Val Loss: 1.5789 | Val Acc: 82.50%\n",
      "Batch loss: 0.0013000450562685728\n",
      "Batch loss: 0.049542177468538284\n",
      "Batch loss: 0.02746819332242012\n",
      "Batch loss: 0.013454284518957138\n",
      "Batch loss: 0.012912172824144363\n",
      "Epoch 72 | Train Loss: 0.0209 | Train Acc: 98.75% | Val Loss: 1.4650 | Val Acc: 85.00%\n",
      "Batch loss: 0.03732473775744438\n",
      "Batch loss: 0.03763563930988312\n",
      "Batch loss: 0.0028584785759449005\n",
      "Batch loss: 0.058485809713602066\n",
      "Batch loss: 0.038821738213300705\n",
      "Epoch 73 | Train Loss: 0.0350 | Train Acc: 98.75% | Val Loss: 1.2618 | Val Acc: 92.50%\n",
      "Batch loss: 0.010610708966851234\n",
      "Batch loss: 0.006115237716585398\n",
      "Batch loss: 0.025802263990044594\n",
      "Batch loss: 0.02352982759475708\n",
      "Batch loss: 0.026343151926994324\n",
      "Epoch 74 | Train Loss: 0.0185 | Train Acc: 98.75% | Val Loss: 1.1759 | Val Acc: 92.50%\n",
      "Batch loss: 0.025407398119568825\n",
      "Batch loss: 0.005016899202018976\n",
      "Batch loss: 0.010133950039744377\n",
      "Batch loss: 0.0010781108867377043\n",
      "Batch loss: 0.04485032334923744\n",
      "Epoch 75 | Train Loss: 0.0173 | Train Acc: 98.75% | Val Loss: 1.1936 | Val Acc: 92.50%\n",
      "Batch loss: 0.005042055621743202\n",
      "Batch loss: 0.07873248308897018\n",
      "Batch loss: 0.002335046650841832\n",
      "Batch loss: 0.03447374328970909\n",
      "Batch loss: 0.0008789290441200137\n",
      "Epoch 76 | Train Loss: 0.0243 | Train Acc: 98.12% | Val Loss: 1.2519 | Val Acc: 92.50%\n",
      "Batch loss: 0.04935925081372261\n",
      "Batch loss: 0.0029444056563079357\n",
      "Batch loss: 0.02481258474290371\n",
      "Batch loss: 0.0036831942852586508\n",
      "Batch loss: 0.027898788452148438\n",
      "Epoch 77 | Train Loss: 0.0217 | Train Acc: 98.75% | Val Loss: 1.1620 | Val Acc: 92.50%\n",
      "Batch loss: 0.021716205403208733\n",
      "Batch loss: 0.008308096788823605\n",
      "Batch loss: 0.0016032225685194135\n",
      "Batch loss: 0.0020420250948518515\n",
      "Batch loss: 0.029657477512955666\n",
      "Epoch 78 | Train Loss: 0.0127 | Train Acc: 100.00% | Val Loss: 1.1123 | Val Acc: 92.50%\n",
      "Batch loss: 0.0021202065981924534\n",
      "Batch loss: 0.0009609318803995848\n",
      "Batch loss: 0.02186518907546997\n",
      "Batch loss: 0.058901313692331314\n",
      "Batch loss: 0.022144299000501633\n",
      "Epoch 79 | Train Loss: 0.0212 | Train Acc: 99.38% | Val Loss: 1.1787 | Val Acc: 92.50%\n",
      "Batch loss: 0.0036006071604788303\n",
      "Batch loss: 0.03907989710569382\n",
      "Batch loss: 0.036578088998794556\n",
      "Batch loss: 0.0013121564406901598\n",
      "Batch loss: 0.008721912279725075\n",
      "Epoch 80 | Train Loss: 0.0179 | Train Acc: 99.38% | Val Loss: 1.3414 | Val Acc: 90.00%\n",
      "Batch loss: 0.04209502413868904\n",
      "Batch loss: 0.021619388833642006\n",
      "Batch loss: 0.023310847580432892\n",
      "Batch loss: 0.005089757032692432\n",
      "Batch loss: 0.0421723872423172\n",
      "Epoch 81 | Train Loss: 0.0269 | Train Acc: 98.75% | Val Loss: 1.2399 | Val Acc: 92.50%\n",
      "Batch loss: 0.024597179144620895\n",
      "Batch loss: 0.0009018857381306589\n",
      "Batch loss: 0.021055085584521294\n",
      "Batch loss: 0.021234456449747086\n",
      "Batch loss: 0.01191654335707426\n",
      "Epoch 82 | Train Loss: 0.0159 | Train Acc: 100.00% | Val Loss: 1.0476 | Val Acc: 92.50%\n",
      "Batch loss: 0.001252592308446765\n",
      "Batch loss: 0.033044733107089996\n",
      "Batch loss: 0.0012776113580912352\n",
      "Batch loss: 0.029408525675535202\n",
      "Batch loss: 0.006493257358670235\n",
      "Epoch 83 | Train Loss: 0.0143 | Train Acc: 98.75% | Val Loss: 0.9488 | Val Acc: 90.00%\n",
      "Batch loss: 0.001786665292456746\n",
      "Batch loss: 0.024810655042529106\n",
      "Batch loss: 0.03580981120467186\n",
      "Batch loss: 0.09403415769338608\n",
      "Batch loss: 0.003026527352631092\n",
      "Epoch 84 | Train Loss: 0.0319 | Train Acc: 98.12% | Val Loss: 1.2269 | Val Acc: 92.50%\n",
      "Batch loss: 0.0023080736864358187\n",
      "Batch loss: 0.0070732939057052135\n",
      "Batch loss: 0.048373837023973465\n",
      "Batch loss: 0.02333073318004608\n",
      "Batch loss: 0.016325214877724648\n",
      "Epoch 85 | Train Loss: 0.0195 | Train Acc: 98.75% | Val Loss: 1.3989 | Val Acc: 92.50%\n",
      "Batch loss: 0.02840110845863819\n",
      "Batch loss: 0.0034084063954651356\n",
      "Batch loss: 0.022754380479454994\n",
      "Batch loss: 0.1317533254623413\n",
      "Batch loss: 0.0005403673276305199\n",
      "Epoch 86 | Train Loss: 0.0374 | Train Acc: 98.75% | Val Loss: 1.1432 | Val Acc: 92.50%\n",
      "Batch loss: 0.03427943214774132\n",
      "Batch loss: 0.05013590678572655\n",
      "Batch loss: 0.004716251045465469\n",
      "Batch loss: 0.004082074388861656\n",
      "Batch loss: 0.023317474871873856\n",
      "Epoch 87 | Train Loss: 0.0233 | Train Acc: 97.50% | Val Loss: 0.7557 | Val Acc: 95.00%\n",
      "Batch loss: 0.04879314824938774\n",
      "Batch loss: 0.003380588721483946\n",
      "Batch loss: 0.035293612629175186\n",
      "Batch loss: 0.1254674643278122\n",
      "Batch loss: 0.06541892886161804\n",
      "Epoch 88 | Train Loss: 0.0557 | Train Acc: 96.25% | Val Loss: 0.8675 | Val Acc: 92.50%\n",
      "Batch loss: 0.02964804694056511\n",
      "Batch loss: 0.014048267155885696\n",
      "Batch loss: 0.0023969626054167747\n",
      "Batch loss: 0.029284272342920303\n",
      "Batch loss: 0.004608089104294777\n",
      "Epoch 89 | Train Loss: 0.0160 | Train Acc: 98.75% | Val Loss: 1.1191 | Val Acc: 92.50%\n",
      "Batch loss: 0.0006666644476354122\n",
      "Batch loss: 0.01670854166150093\n",
      "Batch loss: 0.09522639214992523\n",
      "Batch loss: 0.023082101717591286\n",
      "Batch loss: 0.005363799165934324\n",
      "Epoch 90 | Train Loss: 0.0282 | Train Acc: 97.50% | Val Loss: 1.2707 | Val Acc: 92.50%\n",
      "Batch loss: 0.0250668004155159\n",
      "Batch loss: 0.012009266763925552\n",
      "Batch loss: 0.020781921222805977\n",
      "Batch loss: 0.025238512083888054\n",
      "Batch loss: 0.014522338286042213\n",
      "Epoch 91 | Train Loss: 0.0195 | Train Acc: 98.75% | Val Loss: 1.3917 | Val Acc: 92.50%\n",
      "Batch loss: 0.04134691506624222\n",
      "Batch loss: 0.00239258143119514\n",
      "Batch loss: 0.06660708039999008\n",
      "Batch loss: 0.028473781421780586\n",
      "Batch loss: 0.024229781702160835\n",
      "Epoch 92 | Train Loss: 0.0326 | Train Acc: 98.12% | Val Loss: 1.5338 | Val Acc: 90.00%\n",
      "Batch loss: 0.0009074065601453185\n",
      "Batch loss: 0.20767147839069366\n",
      "Batch loss: 0.004912598058581352\n",
      "Batch loss: 0.02231457643210888\n",
      "Batch loss: 0.035243358463048935\n",
      "Epoch 93 | Train Loss: 0.0542 | Train Acc: 97.50% | Val Loss: 0.7141 | Val Acc: 85.00%\n",
      "Batch loss: 0.052499670535326004\n",
      "Batch loss: 0.030980585142970085\n",
      "Batch loss: 0.2346048504114151\n",
      "Batch loss: 0.06766432523727417\n",
      "Batch loss: 0.01933007501065731\n",
      "Epoch 94 | Train Loss: 0.0810 | Train Acc: 95.62% | Val Loss: 0.7988 | Val Acc: 92.50%\n",
      "Batch loss: 0.0293709859251976\n",
      "Batch loss: 0.03673973307013512\n",
      "Batch loss: 0.02587665058672428\n",
      "Batch loss: 0.08263495564460754\n",
      "Batch loss: 0.2036878913640976\n",
      "Epoch 95 | Train Loss: 0.0757 | Train Acc: 96.88% | Val Loss: 1.5873 | Val Acc: 87.50%\n",
      "Batch loss: 0.03122403286397457\n",
      "Batch loss: 0.05223481357097626\n",
      "Batch loss: 0.014432698488235474\n",
      "Batch loss: 0.060269035398960114\n",
      "Batch loss: 0.022916994988918304\n",
      "Epoch 96 | Train Loss: 0.0362 | Train Acc: 98.12% | Val Loss: 1.4402 | Val Acc: 92.50%\n",
      "Batch loss: 0.0885654017329216\n",
      "Batch loss: 0.033428341150283813\n",
      "Batch loss: 0.0234833974391222\n",
      "Batch loss: 0.0023690799716860056\n",
      "Batch loss: 0.0993594080209732\n",
      "Epoch 97 | Train Loss: 0.0494 | Train Acc: 96.88% | Val Loss: 1.1471 | Val Acc: 90.00%\n",
      "Batch loss: 0.06306853890419006\n",
      "Batch loss: 0.003247656626626849\n",
      "Batch loss: 0.0031934829894453287\n",
      "Batch loss: 0.04915744438767433\n",
      "Batch loss: 0.05406101047992706\n",
      "Epoch 98 | Train Loss: 0.0345 | Train Acc: 97.50% | Val Loss: 1.1095 | Val Acc: 92.50%\n",
      "Batch loss: 0.001778982114046812\n",
      "Batch loss: 0.00519513851031661\n",
      "Batch loss: 0.0439818799495697\n",
      "Batch loss: 0.024113399907946587\n",
      "Batch loss: 0.023243563249707222\n",
      "Epoch 99 | Train Loss: 0.0197 | Train Acc: 98.12% | Val Loss: 1.2407 | Val Acc: 92.50%\n",
      "Batch loss: 0.034543100744485855\n",
      "Batch loss: 0.09141046553850174\n",
      "Batch loss: 0.06892600655555725\n",
      "Batch loss: 0.04882638528943062\n",
      "Batch loss: 0.04196808859705925\n",
      "Epoch 100 | Train Loss: 0.0571 | Train Acc: 96.25% | Val Loss: 1.3188 | Val Acc: 92.50%\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def train_model(epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            print(f\"Batch loss: {loss.item()}\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "# Assuming you already have a val_loader for your validation dataset\n",
    "\n",
    "train_model(epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319ad0f",
   "metadata": {},
   "source": [
    "Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6aa46f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_single_image(image_path, model, class_names):\n",
    "    model.eval()\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        probs = F.softmax(output, dim=1)\n",
    "        _, predicted = torch.max(probs, 1)\n",
    "\n",
    "    print(f\"Predicted Class: {class_names[predicted.item()]}\")\n",
    "    print(f\"Class Probabilities: {probs.squeeze().numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdd2aac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: serie_healthy_leaves_augmented\n",
      "Class Probabilities: [9.991443e-01 8.556470e-04]\n"
     ]
    }
   ],
   "source": [
    "# Assuming dataset = ImageFolder(...)\n",
    "class_names = dataset.classes  # ['healthy', 'infected']\n",
    "\n",
    "# Path to one test image\n",
    "test_image_path_1 = \"processed_data/serie healthy leaves/healthy_04.png\"\n",
    "\n",
    "predict_single_image(test_image_path_1, model, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b1c40fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: series_infected_leaves_augmented\n",
      "Class Probabilities: [1.7830813e-29 1.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "test_image_path_2 = \"processed_data/serie infected leaves/infected_05.png\"\n",
    "predict_single_image(test_image_path_2, model, class_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
