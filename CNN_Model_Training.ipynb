{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b25eb04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaae90d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder('aug_processed_data', transform=transform)\n",
    "\n",
    "# Train/val split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39fbb19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 32 * 32, 64)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))   # 64x64\n",
    "        x = self.pool(F.relu(self.conv2(x)))   # 32x32\n",
    "        x = x.view(-1, 32 * 32 * 32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da11de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Approch 2\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Dropout with 50% probability\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * 32 * 32, 64)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))   # 128->64\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))   # 64->32\n",
    "\n",
    "        x = x.view(-1, 32 * 32 * 32)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))            # Dropout before fc2\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "946f0ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # weight_decay added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "043a7e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.8601464033126831\n",
      "Batch loss: 18.905698776245117\n",
      "Batch loss: 2.4714510440826416\n",
      "Batch loss: 4.326797008514404\n",
      "Batch loss: 4.848811149597168\n",
      "Epoch 1 | Train Loss: 6.2826 | Train Acc: 45.00% | Val Loss: 0.6796 | Val Acc: 55.00%\n",
      "Batch loss: 3.625509738922119\n",
      "Batch loss: 1.2845743894577026\n",
      "Batch loss: 1.8352090120315552\n",
      "Batch loss: 1.3428250551223755\n",
      "Batch loss: 1.290202021598816\n",
      "Epoch 2 | Train Loss: 1.8757 | Train Acc: 61.25% | Val Loss: 0.4080 | Val Acc: 75.00%\n",
      "Batch loss: 0.4759811758995056\n",
      "Batch loss: 1.0756744146347046\n",
      "Batch loss: 1.5735763311386108\n",
      "Batch loss: 1.4803065061569214\n",
      "Batch loss: 1.1951477527618408\n",
      "Epoch 3 | Train Loss: 1.1601 | Train Acc: 58.75% | Val Loss: 0.3801 | Val Acc: 72.50%\n",
      "Batch loss: 0.7290656566619873\n",
      "Batch loss: 0.4212764799594879\n",
      "Batch loss: 0.5841568112373352\n",
      "Batch loss: 0.4425114393234253\n",
      "Batch loss: 0.3505750000476837\n",
      "Epoch 4 | Train Loss: 0.5055 | Train Acc: 62.50% | Val Loss: 0.4386 | Val Acc: 45.00%\n",
      "Batch loss: 0.48222529888153076\n",
      "Batch loss: 0.5836013555526733\n",
      "Batch loss: 0.5018565654754639\n",
      "Batch loss: 0.4299352765083313\n",
      "Batch loss: 0.5095989108085632\n",
      "Epoch 5 | Train Loss: 0.5014 | Train Acc: 55.00% | Val Loss: 0.4481 | Val Acc: 45.00%\n",
      "Batch loss: 0.49124571681022644\n",
      "Batch loss: 0.5393678545951843\n",
      "Batch loss: 0.44341620802879333\n",
      "Batch loss: 0.45405223965644836\n",
      "Batch loss: 0.43299445509910583\n",
      "Epoch 6 | Train Loss: 0.4722 | Train Acc: 68.75% | Val Loss: 0.5055 | Val Acc: 65.00%\n",
      "Batch loss: 0.5951693058013916\n",
      "Batch loss: 0.42836445569992065\n",
      "Batch loss: 0.42904478311538696\n",
      "Batch loss: 0.3044743537902832\n",
      "Batch loss: 0.3835439383983612\n",
      "Epoch 7 | Train Loss: 0.4281 | Train Acc: 81.88% | Val Loss: 0.3765 | Val Acc: 77.50%\n",
      "Batch loss: 0.3056066334247589\n",
      "Batch loss: 0.3812093734741211\n",
      "Batch loss: 0.3335234522819519\n",
      "Batch loss: 0.3432462215423584\n",
      "Batch loss: 0.46289390325546265\n",
      "Epoch 8 | Train Loss: 0.3653 | Train Acc: 83.12% | Val Loss: 0.4624 | Val Acc: 77.50%\n",
      "Batch loss: 0.4040878415107727\n",
      "Batch loss: 0.3191271126270294\n",
      "Batch loss: 0.34587010741233826\n",
      "Batch loss: 0.4194258153438568\n",
      "Batch loss: 0.2804827392101288\n",
      "Epoch 9 | Train Loss: 0.3538 | Train Acc: 80.62% | Val Loss: 0.3603 | Val Acc: 75.00%\n",
      "Batch loss: 0.31897565722465515\n",
      "Batch loss: 0.6903024315834045\n",
      "Batch loss: 0.2984214127063751\n",
      "Batch loss: 0.3716810643672943\n",
      "Batch loss: 0.3313916325569153\n",
      "Epoch 10 | Train Loss: 0.4022 | Train Acc: 85.62% | Val Loss: 0.4121 | Val Acc: 87.50%\n",
      "Batch loss: 0.3890354335308075\n",
      "Batch loss: 0.3363494575023651\n",
      "Batch loss: 0.3129649758338928\n",
      "Batch loss: 0.32294437289237976\n",
      "Batch loss: 0.2912253141403198\n",
      "Epoch 11 | Train Loss: 0.3305 | Train Acc: 85.00% | Val Loss: 0.4926 | Val Acc: 87.50%\n",
      "Batch loss: 0.35392236709594727\n",
      "Batch loss: 0.3122949004173279\n",
      "Batch loss: 0.4116576015949249\n",
      "Batch loss: 0.22788967192173004\n",
      "Batch loss: 0.29601410031318665\n",
      "Epoch 12 | Train Loss: 0.3204 | Train Acc: 81.88% | Val Loss: 0.6110 | Val Acc: 80.00%\n",
      "Batch loss: 0.22432352602481842\n",
      "Batch loss: 0.4404159486293793\n",
      "Batch loss: 0.3218657672405243\n",
      "Batch loss: 0.30723056197166443\n",
      "Batch loss: 0.3461581766605377\n",
      "Epoch 13 | Train Loss: 0.3280 | Train Acc: 85.62% | Val Loss: 0.5435 | Val Acc: 82.50%\n",
      "Batch loss: 0.3123716413974762\n",
      "Batch loss: 0.23254114389419556\n",
      "Batch loss: 0.3031157851219177\n",
      "Batch loss: 0.3270973563194275\n",
      "Batch loss: 0.33578556776046753\n",
      "Epoch 14 | Train Loss: 0.3022 | Train Acc: 86.88% | Val Loss: 0.4580 | Val Acc: 80.00%\n",
      "Batch loss: 0.32085463404655457\n",
      "Batch loss: 0.26270076632499695\n",
      "Batch loss: 0.3480035960674286\n",
      "Batch loss: 0.29797443747520447\n",
      "Batch loss: 0.26291581988334656\n",
      "Epoch 15 | Train Loss: 0.2985 | Train Acc: 86.25% | Val Loss: 0.4543 | Val Acc: 82.50%\n",
      "Batch loss: 0.230805441737175\n",
      "Batch loss: 0.271033376455307\n",
      "Batch loss: 0.25693681836128235\n",
      "Batch loss: 0.2910814583301544\n",
      "Batch loss: 0.25201115012168884\n",
      "Epoch 16 | Train Loss: 0.2604 | Train Acc: 90.62% | Val Loss: 0.4545 | Val Acc: 87.50%\n",
      "Batch loss: 0.37321585416793823\n",
      "Batch loss: 0.270233690738678\n",
      "Batch loss: 0.17246384918689728\n",
      "Batch loss: 0.38656216859817505\n",
      "Batch loss: 0.21575161814689636\n",
      "Epoch 17 | Train Loss: 0.2836 | Train Acc: 91.88% | Val Loss: 0.4878 | Val Acc: 85.00%\n",
      "Batch loss: 0.215830996632576\n",
      "Batch loss: 0.3364979922771454\n",
      "Batch loss: 0.22912263870239258\n",
      "Batch loss: 0.2671786844730377\n",
      "Batch loss: 0.36556944251060486\n",
      "Epoch 18 | Train Loss: 0.2828 | Train Acc: 85.62% | Val Loss: 0.5047 | Val Acc: 87.50%\n",
      "Batch loss: 0.1540370136499405\n",
      "Batch loss: 0.29184576869010925\n",
      "Batch loss: 0.24390698969364166\n",
      "Batch loss: 0.3869955539703369\n",
      "Batch loss: 0.1874244213104248\n",
      "Epoch 19 | Train Loss: 0.2528 | Train Acc: 90.62% | Val Loss: 0.6179 | Val Acc: 80.00%\n",
      "Batch loss: 0.2724117040634155\n",
      "Batch loss: 0.3505701422691345\n",
      "Batch loss: 0.2878223955631256\n",
      "Batch loss: 0.26939645409584045\n",
      "Batch loss: 0.3458866477012634\n",
      "Epoch 20 | Train Loss: 0.3052 | Train Acc: 89.38% | Val Loss: 0.5009 | Val Acc: 85.00%\n",
      "Batch loss: 0.21303384006023407\n",
      "Batch loss: 0.18266592919826508\n",
      "Batch loss: 0.26404815912246704\n",
      "Batch loss: 0.2317611277103424\n",
      "Batch loss: 0.18488797545433044\n",
      "Epoch 21 | Train Loss: 0.2153 | Train Acc: 90.00% | Val Loss: 0.4087 | Val Acc: 87.50%\n",
      "Batch loss: 0.29870688915252686\n",
      "Batch loss: 0.24081550538539886\n",
      "Batch loss: 0.2546015679836273\n",
      "Batch loss: 0.2546917796134949\n",
      "Batch loss: 0.21180057525634766\n",
      "Epoch 22 | Train Loss: 0.2521 | Train Acc: 91.88% | Val Loss: 0.3704 | Val Acc: 90.00%\n",
      "Batch loss: 0.2275083363056183\n",
      "Batch loss: 0.33692875504493713\n",
      "Batch loss: 0.1639356017112732\n",
      "Batch loss: 0.19293467700481415\n",
      "Batch loss: 0.20678067207336426\n",
      "Epoch 23 | Train Loss: 0.2256 | Train Acc: 93.12% | Val Loss: 0.3618 | Val Acc: 87.50%\n",
      "Batch loss: 0.24445785582065582\n",
      "Batch loss: 0.1753021627664566\n",
      "Batch loss: 0.1900966912508011\n",
      "Batch loss: 0.23512068390846252\n",
      "Batch loss: 0.1047009527683258\n",
      "Epoch 24 | Train Loss: 0.1899 | Train Acc: 96.25% | Val Loss: 0.4387 | Val Acc: 85.00%\n",
      "Batch loss: 0.16274479031562805\n",
      "Batch loss: 0.3062151074409485\n",
      "Batch loss: 0.1922033429145813\n",
      "Batch loss: 0.23502126336097717\n",
      "Batch loss: 0.1625061184167862\n",
      "Epoch 25 | Train Loss: 0.2117 | Train Acc: 95.00% | Val Loss: 0.5188 | Val Acc: 85.00%\n",
      "Batch loss: 0.19903194904327393\n",
      "Batch loss: 0.17645792663097382\n",
      "Batch loss: 0.2751409113407135\n",
      "Batch loss: 0.2126741111278534\n",
      "Batch loss: 0.19573494791984558\n",
      "Epoch 26 | Train Loss: 0.2118 | Train Acc: 93.12% | Val Loss: 0.5465 | Val Acc: 82.50%\n",
      "Batch loss: 0.24218986928462982\n",
      "Batch loss: 0.2770759165287018\n",
      "Batch loss: 0.4330688714981079\n",
      "Batch loss: 0.20814937353134155\n",
      "Batch loss: 0.1949625462293625\n",
      "Epoch 27 | Train Loss: 0.2711 | Train Acc: 89.38% | Val Loss: 0.5390 | Val Acc: 85.00%\n",
      "Batch loss: 0.14641450345516205\n",
      "Batch loss: 0.1503572016954422\n",
      "Batch loss: 0.2827882468700409\n",
      "Batch loss: 0.168876513838768\n",
      "Batch loss: 0.1896989494562149\n",
      "Epoch 28 | Train Loss: 0.1876 | Train Acc: 94.38% | Val Loss: 0.6066 | Val Acc: 82.50%\n",
      "Batch loss: 0.17356324195861816\n",
      "Batch loss: 0.30390191078186035\n",
      "Batch loss: 0.1534246951341629\n",
      "Batch loss: 0.15050213038921356\n",
      "Batch loss: 0.27054351568222046\n",
      "Epoch 29 | Train Loss: 0.2104 | Train Acc: 96.25% | Val Loss: 0.4967 | Val Acc: 82.50%\n",
      "Batch loss: 0.19833123683929443\n",
      "Batch loss: 0.20723828673362732\n",
      "Batch loss: 0.23622916638851166\n",
      "Batch loss: 0.2637399733066559\n",
      "Batch loss: 0.23070421814918518\n",
      "Epoch 30 | Train Loss: 0.2272 | Train Acc: 90.62% | Val Loss: 0.3838 | Val Acc: 85.00%\n",
      "Batch loss: 0.1891532987356186\n",
      "Batch loss: 0.24602775275707245\n",
      "Batch loss: 0.16911178827285767\n",
      "Batch loss: 0.1520797610282898\n",
      "Batch loss: 0.1685599684715271\n",
      "Epoch 31 | Train Loss: 0.1850 | Train Acc: 96.25% | Val Loss: 0.3408 | Val Acc: 90.00%\n",
      "Batch loss: 0.22450672090053558\n",
      "Batch loss: 0.24823354184627533\n",
      "Batch loss: 0.22676905989646912\n",
      "Batch loss: 0.2733071446418762\n",
      "Batch loss: 0.1984989196062088\n",
      "Epoch 32 | Train Loss: 0.2343 | Train Acc: 91.88% | Val Loss: 0.4373 | Val Acc: 85.00%\n",
      "Batch loss: 0.13888464868068695\n",
      "Batch loss: 0.18920737504959106\n",
      "Batch loss: 0.29829472303390503\n",
      "Batch loss: 0.18941932916641235\n",
      "Batch loss: 0.19039323925971985\n",
      "Epoch 33 | Train Loss: 0.2012 | Train Acc: 94.38% | Val Loss: 0.4615 | Val Acc: 85.00%\n",
      "Batch loss: 0.2144254744052887\n",
      "Batch loss: 0.2495059072971344\n",
      "Batch loss: 0.1757526844739914\n",
      "Batch loss: 0.2539421617984772\n",
      "Batch loss: 0.16283360123634338\n",
      "Epoch 34 | Train Loss: 0.2113 | Train Acc: 96.25% | Val Loss: 0.5771 | Val Acc: 85.00%\n",
      "Batch loss: 0.1675325334072113\n",
      "Batch loss: 0.6173503994941711\n",
      "Batch loss: 0.10045476257801056\n",
      "Batch loss: 0.19003047049045563\n",
      "Batch loss: 0.1375172734260559\n",
      "Epoch 35 | Train Loss: 0.2426 | Train Acc: 92.50% | Val Loss: 0.6372 | Val Acc: 82.50%\n",
      "Batch loss: 0.19777974486351013\n",
      "Batch loss: 0.22881105542182922\n",
      "Batch loss: 0.18720456957817078\n",
      "Batch loss: 0.23480267822742462\n",
      "Batch loss: 0.20040270686149597\n",
      "Epoch 36 | Train Loss: 0.2098 | Train Acc: 91.88% | Val Loss: 0.6825 | Val Acc: 77.50%\n",
      "Batch loss: 0.20666657388210297\n",
      "Batch loss: 0.1819303333759308\n",
      "Batch loss: 0.17811442911624908\n",
      "Batch loss: 0.2575013041496277\n",
      "Batch loss: 0.16559456288814545\n",
      "Epoch 37 | Train Loss: 0.1980 | Train Acc: 95.62% | Val Loss: 0.6484 | Val Acc: 80.00%\n",
      "Batch loss: 0.24559247493743896\n",
      "Batch loss: 0.19557377696037292\n",
      "Batch loss: 0.21494382619857788\n",
      "Batch loss: 0.21221289038658142\n",
      "Batch loss: 0.20343711972236633\n",
      "Epoch 38 | Train Loss: 0.2144 | Train Acc: 95.62% | Val Loss: 0.6654 | Val Acc: 82.50%\n",
      "Batch loss: 0.45229238271713257\n",
      "Batch loss: 0.17892055213451385\n",
      "Batch loss: 0.16426126658916473\n",
      "Batch loss: 0.20501279830932617\n",
      "Batch loss: 0.18599940836429596\n",
      "Epoch 39 | Train Loss: 0.2373 | Train Acc: 93.75% | Val Loss: 0.6412 | Val Acc: 82.50%\n",
      "Batch loss: 0.2663402259349823\n",
      "Batch loss: 0.196223646402359\n",
      "Batch loss: 0.13051347434520721\n",
      "Batch loss: 0.22042202949523926\n",
      "Batch loss: 0.21765562891960144\n",
      "Epoch 40 | Train Loss: 0.2062 | Train Acc: 91.88% | Val Loss: 0.5857 | Val Acc: 87.50%\n",
      "Batch loss: 0.22942158579826355\n",
      "Batch loss: 0.17467215657234192\n",
      "Batch loss: 0.21744099259376526\n",
      "Batch loss: 0.17284226417541504\n",
      "Batch loss: 0.10386672616004944\n",
      "Epoch 41 | Train Loss: 0.1796 | Train Acc: 96.88% | Val Loss: 0.5638 | Val Acc: 82.50%\n",
      "Batch loss: 0.12374059855937958\n",
      "Batch loss: 0.16795581579208374\n",
      "Batch loss: 0.11827804148197174\n",
      "Batch loss: 0.24304938316345215\n",
      "Batch loss: 0.24435316026210785\n",
      "Epoch 42 | Train Loss: 0.1795 | Train Acc: 91.88% | Val Loss: 0.5568 | Val Acc: 82.50%\n",
      "Batch loss: 0.31375622749328613\n",
      "Batch loss: 0.19625838100910187\n",
      "Batch loss: 0.20501668751239777\n",
      "Batch loss: 0.18849875032901764\n",
      "Batch loss: 0.1629256159067154\n",
      "Epoch 43 | Train Loss: 0.2133 | Train Acc: 92.50% | Val Loss: 0.4429 | Val Acc: 85.00%\n",
      "Batch loss: 0.1778547316789627\n",
      "Batch loss: 0.29406726360321045\n",
      "Batch loss: 0.20561236143112183\n",
      "Batch loss: 0.12133929878473282\n",
      "Batch loss: 0.148126021027565\n",
      "Epoch 44 | Train Loss: 0.1894 | Train Acc: 92.50% | Val Loss: 0.3983 | Val Acc: 85.00%\n",
      "Batch loss: 0.30874738097190857\n",
      "Batch loss: 0.1977100372314453\n",
      "Batch loss: 0.17667673528194427\n",
      "Batch loss: 0.21302898228168488\n",
      "Batch loss: 0.21566390991210938\n",
      "Epoch 45 | Train Loss: 0.2224 | Train Acc: 94.38% | Val Loss: 0.4568 | Val Acc: 85.00%\n",
      "Batch loss: 0.2936311364173889\n",
      "Batch loss: 0.2875063121318817\n",
      "Batch loss: 0.20832742750644684\n",
      "Batch loss: 0.13436417281627655\n",
      "Batch loss: 0.21951152384281158\n",
      "Epoch 46 | Train Loss: 0.2287 | Train Acc: 91.88% | Val Loss: 0.5266 | Val Acc: 87.50%\n",
      "Batch loss: 0.25869521498680115\n",
      "Batch loss: 0.18636171519756317\n",
      "Batch loss: 0.17395535111427307\n",
      "Batch loss: 0.19758421182632446\n",
      "Batch loss: 0.15018509328365326\n",
      "Epoch 47 | Train Loss: 0.1934 | Train Acc: 92.50% | Val Loss: 0.5690 | Val Acc: 85.00%\n",
      "Batch loss: 0.18209746479988098\n",
      "Batch loss: 0.23052121698856354\n",
      "Batch loss: 0.21588134765625\n",
      "Batch loss: 0.16090074181556702\n",
      "Batch loss: 0.23157209157943726\n",
      "Epoch 48 | Train Loss: 0.2042 | Train Acc: 94.38% | Val Loss: 0.6663 | Val Acc: 87.50%\n",
      "Batch loss: 0.1573171317577362\n",
      "Batch loss: 0.19755935668945312\n",
      "Batch loss: 0.11128296703100204\n",
      "Batch loss: 0.2175925076007843\n",
      "Batch loss: 0.22306804358959198\n",
      "Epoch 49 | Train Loss: 0.1814 | Train Acc: 93.12% | Val Loss: 0.7508 | Val Acc: 85.00%\n",
      "Batch loss: 0.19285964965820312\n",
      "Batch loss: 0.21742793917655945\n",
      "Batch loss: 0.23502279818058014\n",
      "Batch loss: 0.31451913714408875\n",
      "Batch loss: 0.34329575300216675\n",
      "Epoch 50 | Train Loss: 0.2606 | Train Acc: 90.62% | Val Loss: 0.6053 | Val Acc: 82.50%\n",
      "Batch loss: 0.1870345175266266\n",
      "Batch loss: 0.12742173671722412\n",
      "Batch loss: 0.13816125690937042\n",
      "Batch loss: 0.1526324450969696\n",
      "Batch loss: 0.19848497211933136\n",
      "Epoch 51 | Train Loss: 0.1607 | Train Acc: 96.25% | Val Loss: 0.4608 | Val Acc: 82.50%\n",
      "Batch loss: 0.12620016932487488\n",
      "Batch loss: 0.21430008113384247\n",
      "Batch loss: 0.15585778653621674\n",
      "Batch loss: 0.18268023431301117\n",
      "Batch loss: 0.1972997635602951\n",
      "Epoch 52 | Train Loss: 0.1753 | Train Acc: 93.75% | Val Loss: 0.4552 | Val Acc: 82.50%\n",
      "Batch loss: 0.1976248174905777\n",
      "Batch loss: 0.2493223249912262\n",
      "Batch loss: 0.3437384366989136\n",
      "Batch loss: 0.23656918108463287\n",
      "Batch loss: 0.2495451271533966\n",
      "Epoch 53 | Train Loss: 0.2554 | Train Acc: 91.88% | Val Loss: 0.4632 | Val Acc: 85.00%\n",
      "Batch loss: 0.17916470766067505\n",
      "Batch loss: 0.2223740816116333\n",
      "Batch loss: 0.11837677657604218\n",
      "Batch loss: 0.16171987354755402\n",
      "Batch loss: 0.15443292260169983\n",
      "Epoch 54 | Train Loss: 0.1672 | Train Acc: 96.88% | Val Loss: 0.5335 | Val Acc: 85.00%\n",
      "Batch loss: 0.2389928251504898\n",
      "Batch loss: 0.14742612838745117\n",
      "Batch loss: 0.24520504474639893\n",
      "Batch loss: 0.1525420844554901\n",
      "Batch loss: 0.19021368026733398\n",
      "Epoch 55 | Train Loss: 0.1949 | Train Acc: 92.50% | Val Loss: 0.5786 | Val Acc: 85.00%\n",
      "Batch loss: 0.18545737862586975\n",
      "Batch loss: 0.16203546524047852\n",
      "Batch loss: 0.18557485938072205\n",
      "Batch loss: 0.16057910025119781\n",
      "Batch loss: 0.19817164540290833\n",
      "Epoch 56 | Train Loss: 0.1784 | Train Acc: 93.12% | Val Loss: 0.5840 | Val Acc: 82.50%\n",
      "Batch loss: 0.2036462426185608\n",
      "Batch loss: 0.19856177270412445\n",
      "Batch loss: 0.19928957521915436\n",
      "Batch loss: 0.19080939888954163\n",
      "Batch loss: 0.16925473511219025\n",
      "Epoch 57 | Train Loss: 0.1923 | Train Acc: 95.00% | Val Loss: 0.5666 | Val Acc: 82.50%\n",
      "Batch loss: 0.22725647687911987\n",
      "Batch loss: 0.16153115034103394\n",
      "Batch loss: 0.1938590109348297\n",
      "Batch loss: 0.2317914366722107\n",
      "Batch loss: 0.19716209173202515\n",
      "Epoch 58 | Train Loss: 0.2023 | Train Acc: 95.62% | Val Loss: 0.5657 | Val Acc: 82.50%\n",
      "Batch loss: 0.1804942637681961\n",
      "Batch loss: 0.19283220171928406\n",
      "Batch loss: 0.23676425218582153\n",
      "Batch loss: 0.07972877472639084\n",
      "Batch loss: 0.20101268589496613\n",
      "Epoch 59 | Train Loss: 0.1782 | Train Acc: 95.00% | Val Loss: 0.5888 | Val Acc: 82.50%\n",
      "Batch loss: 0.18465222418308258\n",
      "Batch loss: 0.20347346365451813\n",
      "Batch loss: 0.16665199398994446\n",
      "Batch loss: 0.20006850361824036\n",
      "Batch loss: 0.17506642639636993\n",
      "Epoch 60 | Train Loss: 0.1860 | Train Acc: 95.00% | Val Loss: 0.6195 | Val Acc: 82.50%\n",
      "Batch loss: 0.1415630728006363\n",
      "Batch loss: 0.20155273377895355\n",
      "Batch loss: 0.19965867698192596\n",
      "Batch loss: 0.21497634053230286\n",
      "Batch loss: 0.20130884647369385\n",
      "Epoch 61 | Train Loss: 0.1918 | Train Acc: 95.00% | Val Loss: 0.6568 | Val Acc: 82.50%\n",
      "Batch loss: 0.22026759386062622\n",
      "Batch loss: 0.15556950867176056\n",
      "Batch loss: 0.14401289820671082\n",
      "Batch loss: 0.19353868067264557\n",
      "Batch loss: 0.19843274354934692\n",
      "Epoch 62 | Train Loss: 0.1824 | Train Acc: 95.00% | Val Loss: 0.6782 | Val Acc: 82.50%\n",
      "Batch loss: 0.1643267571926117\n",
      "Batch loss: 0.152279794216156\n",
      "Batch loss: 0.13649801909923553\n",
      "Batch loss: 0.17357173562049866\n",
      "Batch loss: 0.0876661166548729\n",
      "Epoch 63 | Train Loss: 0.1429 | Train Acc: 97.50% | Val Loss: 0.6792 | Val Acc: 82.50%\n",
      "Batch loss: 0.11409379541873932\n",
      "Batch loss: 0.1254817694425583\n",
      "Batch loss: 0.206509530544281\n",
      "Batch loss: 0.17990818619728088\n",
      "Batch loss: 0.20671190321445465\n",
      "Epoch 64 | Train Loss: 0.1665 | Train Acc: 96.88% | Val Loss: 0.7212 | Val Acc: 82.50%\n",
      "Batch loss: 0.22809186577796936\n",
      "Batch loss: 0.07337166368961334\n",
      "Batch loss: 0.2132430523633957\n",
      "Batch loss: 0.2013443410396576\n",
      "Batch loss: 0.2791942358016968\n",
      "Epoch 65 | Train Loss: 0.1990 | Train Acc: 93.75% | Val Loss: 0.6789 | Val Acc: 85.00%\n",
      "Batch loss: 0.2184354066848755\n",
      "Batch loss: 0.16063573956489563\n",
      "Batch loss: 0.1731584221124649\n",
      "Batch loss: 0.15934839844703674\n",
      "Batch loss: 0.14891351759433746\n",
      "Epoch 66 | Train Loss: 0.1721 | Train Acc: 93.75% | Val Loss: 0.5355 | Val Acc: 85.00%\n",
      "Batch loss: 0.11833415180444717\n",
      "Batch loss: 0.17914584279060364\n",
      "Batch loss: 0.15552683174610138\n",
      "Batch loss: 0.26277804374694824\n",
      "Batch loss: 0.07864563912153244\n",
      "Epoch 67 | Train Loss: 0.1589 | Train Acc: 93.75% | Val Loss: 0.5059 | Val Acc: 85.00%\n",
      "Batch loss: 0.06743839383125305\n",
      "Batch loss: 0.08064290136098862\n",
      "Batch loss: 0.15352492034435272\n",
      "Batch loss: 0.14546562731266022\n",
      "Batch loss: 0.1944657862186432\n",
      "Epoch 68 | Train Loss: 0.1283 | Train Acc: 96.88% | Val Loss: 0.5799 | Val Acc: 87.50%\n",
      "Batch loss: 0.09594229608774185\n",
      "Batch loss: 0.17184557020664215\n",
      "Batch loss: 0.3084416687488556\n",
      "Batch loss: 0.14870916306972504\n",
      "Batch loss: 0.19216729700565338\n",
      "Epoch 69 | Train Loss: 0.1834 | Train Acc: 95.62% | Val Loss: 0.6466 | Val Acc: 85.00%\n",
      "Batch loss: 0.034448157995939255\n",
      "Batch loss: 0.14498281478881836\n",
      "Batch loss: 0.2427617311477661\n",
      "Batch loss: 0.23717132210731506\n",
      "Batch loss: 0.11518310010433197\n",
      "Epoch 70 | Train Loss: 0.1549 | Train Acc: 96.88% | Val Loss: 0.6990 | Val Acc: 85.00%\n",
      "Batch loss: 0.19200293719768524\n",
      "Batch loss: 0.16394904255867004\n",
      "Batch loss: 0.17581665515899658\n",
      "Batch loss: 0.26813143491744995\n",
      "Batch loss: 0.13984660804271698\n",
      "Epoch 71 | Train Loss: 0.1879 | Train Acc: 96.88% | Val Loss: 0.7493 | Val Acc: 85.00%\n",
      "Batch loss: 0.12715402245521545\n",
      "Batch loss: 0.14971280097961426\n",
      "Batch loss: 0.19478565454483032\n",
      "Batch loss: 0.12993669509887695\n",
      "Batch loss: 0.14864802360534668\n",
      "Epoch 72 | Train Loss: 0.1500 | Train Acc: 96.25% | Val Loss: 0.6605 | Val Acc: 82.50%\n",
      "Batch loss: 0.17201045155525208\n",
      "Batch loss: 0.12981748580932617\n",
      "Batch loss: 0.13061141967773438\n",
      "Batch loss: 0.1370857208967209\n",
      "Batch loss: 0.11639366298913956\n",
      "Epoch 73 | Train Loss: 0.1372 | Train Acc: 98.75% | Val Loss: 0.6530 | Val Acc: 80.00%\n",
      "Batch loss: 0.21305838227272034\n",
      "Batch loss: 0.17322421073913574\n",
      "Batch loss: 0.12979793548583984\n",
      "Batch loss: 0.2075449824333191\n",
      "Batch loss: 0.1567562222480774\n",
      "Epoch 74 | Train Loss: 0.1761 | Train Acc: 97.50% | Val Loss: 0.6299 | Val Acc: 85.00%\n",
      "Batch loss: 0.14211706817150116\n",
      "Batch loss: 0.14447955787181854\n",
      "Batch loss: 0.1561872810125351\n",
      "Batch loss: 0.09330736100673676\n",
      "Batch loss: 0.18791544437408447\n",
      "Epoch 75 | Train Loss: 0.1448 | Train Acc: 98.12% | Val Loss: 0.7943 | Val Acc: 87.50%\n",
      "Batch loss: 0.21903926134109497\n",
      "Batch loss: 0.2268315553665161\n",
      "Batch loss: 0.1879207044839859\n",
      "Batch loss: 0.18179279565811157\n",
      "Batch loss: 0.1400555670261383\n",
      "Epoch 76 | Train Loss: 0.1911 | Train Acc: 93.75% | Val Loss: 0.7730 | Val Acc: 82.50%\n",
      "Batch loss: 0.17696551978588104\n",
      "Batch loss: 0.13950209319591522\n",
      "Batch loss: 0.07674767822027206\n",
      "Batch loss: 0.13815826177597046\n",
      "Batch loss: 0.1754063218832016\n",
      "Epoch 77 | Train Loss: 0.1414 | Train Acc: 97.50% | Val Loss: 0.8022 | Val Acc: 82.50%\n",
      "Batch loss: 0.1258358657360077\n",
      "Batch loss: 0.15685436129570007\n",
      "Batch loss: 0.19128087162971497\n",
      "Batch loss: 0.1400863528251648\n",
      "Batch loss: 0.1728910207748413\n",
      "Epoch 78 | Train Loss: 0.1574 | Train Acc: 98.75% | Val Loss: 0.8792 | Val Acc: 82.50%\n",
      "Batch loss: 0.26360225677490234\n",
      "Batch loss: 0.04787644371390343\n",
      "Batch loss: 0.14068278670310974\n",
      "Batch loss: 0.1430932581424713\n",
      "Batch loss: 0.1258811056613922\n",
      "Epoch 79 | Train Loss: 0.1442 | Train Acc: 98.12% | Val Loss: 0.8862 | Val Acc: 82.50%\n",
      "Batch loss: 0.16824230551719666\n",
      "Batch loss: 0.15272817015647888\n",
      "Batch loss: 0.14192146062850952\n",
      "Batch loss: 0.11962160468101501\n",
      "Batch loss: 0.16995173692703247\n",
      "Epoch 80 | Train Loss: 0.1505 | Train Acc: 98.12% | Val Loss: 0.9818 | Val Acc: 85.00%\n",
      "Batch loss: 0.07757672667503357\n",
      "Batch loss: 0.1719154268503189\n",
      "Batch loss: 0.08471792936325073\n",
      "Batch loss: 0.13146735727787018\n",
      "Batch loss: 0.1405028998851776\n",
      "Epoch 81 | Train Loss: 0.1212 | Train Acc: 98.75% | Val Loss: 1.1547 | Val Acc: 85.00%\n",
      "Batch loss: 0.16815893352031708\n",
      "Batch loss: 0.16740897297859192\n",
      "Batch loss: 0.12139628827571869\n",
      "Batch loss: 0.10702762007713318\n",
      "Batch loss: 0.09148622304201126\n",
      "Epoch 82 | Train Loss: 0.1311 | Train Acc: 98.12% | Val Loss: 1.2761 | Val Acc: 82.50%\n",
      "Batch loss: 0.11057154089212418\n",
      "Batch loss: 0.07602051645517349\n",
      "Batch loss: 0.1921074390411377\n",
      "Batch loss: 0.20755134522914886\n",
      "Batch loss: 0.11148547381162643\n",
      "Epoch 83 | Train Loss: 0.1395 | Train Acc: 96.88% | Val Loss: 1.3390 | Val Acc: 80.00%\n",
      "Batch loss: 0.1110932007431984\n",
      "Batch loss: 0.151433065533638\n",
      "Batch loss: 0.2620139718055725\n",
      "Batch loss: 0.06166212260723114\n",
      "Batch loss: 0.12058091908693314\n",
      "Epoch 84 | Train Loss: 0.1414 | Train Acc: 98.12% | Val Loss: 1.0580 | Val Acc: 85.00%\n",
      "Batch loss: 0.1973007172346115\n",
      "Batch loss: 0.15092898905277252\n",
      "Batch loss: 0.12003213167190552\n",
      "Batch loss: 0.13036862015724182\n",
      "Batch loss: 0.07649721205234528\n",
      "Epoch 85 | Train Loss: 0.1350 | Train Acc: 97.50% | Val Loss: 0.8853 | Val Acc: 82.50%\n",
      "Batch loss: 0.09304296225309372\n",
      "Batch loss: 0.030028337612748146\n",
      "Batch loss: 0.10485900193452835\n",
      "Batch loss: 0.16419121623039246\n",
      "Batch loss: 0.1451779305934906\n",
      "Epoch 86 | Train Loss: 0.1075 | Train Acc: 98.75% | Val Loss: 0.8644 | Val Acc: 82.50%\n",
      "Batch loss: 0.14404964447021484\n",
      "Batch loss: 0.22498595714569092\n",
      "Batch loss: 0.2204151749610901\n",
      "Batch loss: 0.24544009566307068\n",
      "Batch loss: 0.1332322210073471\n",
      "Epoch 87 | Train Loss: 0.1936 | Train Acc: 95.00% | Val Loss: 0.8021 | Val Acc: 87.50%\n",
      "Batch loss: 0.11516457051038742\n",
      "Batch loss: 0.19324690103530884\n",
      "Batch loss: 0.08980630338191986\n",
      "Batch loss: 0.1483839452266693\n",
      "Batch loss: 0.19587770104408264\n",
      "Epoch 88 | Train Loss: 0.1485 | Train Acc: 97.50% | Val Loss: 0.8712 | Val Acc: 80.00%\n",
      "Batch loss: 0.15495608747005463\n",
      "Batch loss: 0.11742081493139267\n",
      "Batch loss: 0.19498533010482788\n",
      "Batch loss: 0.13659679889678955\n",
      "Batch loss: 0.1296980381011963\n",
      "Epoch 89 | Train Loss: 0.1467 | Train Acc: 98.12% | Val Loss: 0.8376 | Val Acc: 82.50%\n",
      "Batch loss: 0.2082604169845581\n",
      "Batch loss: 0.0883416086435318\n",
      "Batch loss: 0.08746407181024551\n",
      "Batch loss: 0.18710048496723175\n",
      "Batch loss: 0.058121491223573685\n",
      "Epoch 90 | Train Loss: 0.1259 | Train Acc: 98.12% | Val Loss: 0.9566 | Val Acc: 82.50%\n",
      "Batch loss: 0.08703494817018509\n",
      "Batch loss: 0.1612442433834076\n",
      "Batch loss: 0.058222439140081406\n",
      "Batch loss: 0.10330812633037567\n",
      "Batch loss: 0.1477210819721222\n",
      "Epoch 91 | Train Loss: 0.1115 | Train Acc: 98.75% | Val Loss: 1.0640 | Val Acc: 80.00%\n",
      "Batch loss: 0.0877910777926445\n",
      "Batch loss: 0.1900668442249298\n",
      "Batch loss: 0.20616811513900757\n",
      "Batch loss: 0.13673165440559387\n",
      "Batch loss: 0.12917368113994598\n",
      "Epoch 92 | Train Loss: 0.1500 | Train Acc: 98.12% | Val Loss: 1.0851 | Val Acc: 80.00%\n",
      "Batch loss: 0.23117631673812866\n",
      "Batch loss: 0.11773885786533356\n",
      "Batch loss: 0.08637408167123795\n",
      "Batch loss: 0.0859733298420906\n",
      "Batch loss: 0.12844926118850708\n",
      "Epoch 93 | Train Loss: 0.1299 | Train Acc: 97.50% | Val Loss: 0.8790 | Val Acc: 85.00%\n",
      "Batch loss: 0.17326419055461884\n",
      "Batch loss: 0.09975209832191467\n",
      "Batch loss: 0.11392480134963989\n",
      "Batch loss: 0.15026706457138062\n",
      "Batch loss: 0.056932076811790466\n",
      "Epoch 94 | Train Loss: 0.1188 | Train Acc: 99.38% | Val Loss: 0.7644 | Val Acc: 87.50%\n",
      "Batch loss: 0.14906851947307587\n",
      "Batch loss: 0.11873364448547363\n",
      "Batch loss: 0.17672906816005707\n",
      "Batch loss: 0.11299695074558258\n",
      "Batch loss: 0.2159745842218399\n",
      "Epoch 95 | Train Loss: 0.1547 | Train Acc: 96.25% | Val Loss: 0.6531 | Val Acc: 87.50%\n",
      "Batch loss: 0.10267283767461777\n",
      "Batch loss: 0.10939909517765045\n",
      "Batch loss: 0.10219812393188477\n",
      "Batch loss: 0.08478082716464996\n",
      "Batch loss: 0.14040182530879974\n",
      "Epoch 96 | Train Loss: 0.1079 | Train Acc: 98.75% | Val Loss: 0.6403 | Val Acc: 87.50%\n",
      "Batch loss: 0.09941643476486206\n",
      "Batch loss: 0.08844633400440216\n",
      "Batch loss: 0.19251026213169098\n",
      "Batch loss: 0.09859559684991837\n",
      "Batch loss: 0.14997075498104095\n",
      "Epoch 97 | Train Loss: 0.1258 | Train Acc: 98.12% | Val Loss: 0.7221 | Val Acc: 85.00%\n",
      "Batch loss: 0.17627598345279694\n",
      "Batch loss: 0.11225058883428574\n",
      "Batch loss: 0.11941921710968018\n",
      "Batch loss: 0.14354091882705688\n",
      "Batch loss: 0.10164889693260193\n",
      "Epoch 98 | Train Loss: 0.1306 | Train Acc: 96.88% | Val Loss: 0.7818 | Val Acc: 85.00%\n",
      "Batch loss: 0.09987924247980118\n",
      "Batch loss: 0.12906180322170258\n",
      "Batch loss: 0.12426465004682541\n",
      "Batch loss: 0.24261796474456787\n",
      "Batch loss: 0.1993122547864914\n",
      "Epoch 99 | Train Loss: 0.1590 | Train Acc: 97.50% | Val Loss: 0.7493 | Val Acc: 85.00%\n",
      "Batch loss: 0.13555274903774261\n",
      "Batch loss: 0.12788884341716766\n",
      "Batch loss: 0.1513870358467102\n",
      "Batch loss: 0.10970556735992432\n",
      "Batch loss: 0.1247968077659607\n",
      "Epoch 100 | Train Loss: 0.1299 | Train Acc: 97.50% | Val Loss: 0.7655 | Val Acc: 85.00%\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def train_model(epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            print(f\"Batch loss: {loss.item()}\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "# Assuming you already have a val_loader for your validation dataset\n",
    "\n",
    "train_model(epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319ad0f",
   "metadata": {},
   "source": [
    "Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6aa46f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_single_image(image_path, model, class_names):\n",
    "    model.eval()\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        probs = F.softmax(output, dim=1)\n",
    "        _, predicted = torch.max(probs, 1)\n",
    "\n",
    "    print(f\"Predicted Class: {class_names[predicted.item()]}\")\n",
    "    print(f\"Class Probabilities: {probs.squeeze().numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdd2aac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: serie_healthy_leaves_augmented\n",
      "Class Probabilities: [9.991443e-01 8.556470e-04]\n"
     ]
    }
   ],
   "source": [
    "# Assuming dataset = ImageFolder(...)\n",
    "class_names = dataset.classes  # ['healthy', 'infected']\n",
    "\n",
    "# Path to one test image\n",
    "test_image_path_1 = \"processed_data/serie healthy leaves/healthy_04.png\"\n",
    "\n",
    "predict_single_image(test_image_path_1, model, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b1c40fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: series_infected_leaves_augmented\n",
      "Class Probabilities: [1.7830813e-29 1.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "test_image_path_2 = \"processed_data/serie infected leaves/infected_05.png\"\n",
    "predict_single_image(test_image_path_2, model, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b42cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
