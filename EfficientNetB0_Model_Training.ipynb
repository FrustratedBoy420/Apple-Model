{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "19e7971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f919bd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PARAMETERS ===\n",
    "root_folder = 'aug_processed_data'  # Folder with 'healthy' and 'unhealthy' subfolders\n",
    "base_dir = 'split_data_EfficientNetB0'                        # New folder where train/val split will go\n",
    "train_ratio = 0.8                              # 80% train, 20% val split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9797257",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder('aug_processed_data', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d74a05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Create train/val folders with same subfolder structure ===\n",
    "def create_train_val_split(root_folder, base_dir, train_ratio=0.8):\n",
    "    if os.path.exists(base_dir):\n",
    "        shutil.rmtree(base_dir)\n",
    "    os.makedirs(base_dir)\n",
    "\n",
    "    train_dir = os.path.join(base_dir, 'train')\n",
    "    val_dir = os.path.join(base_dir, 'val')\n",
    "    os.makedirs(train_dir)\n",
    "    os.makedirs(val_dir)\n",
    "\n",
    "    classes = [d for d in os.listdir(root_folder) if os.path.isdir(os.path.join(root_folder, d))]\n",
    "\n",
    "    for cls in classes:\n",
    "        os.makedirs(os.path.join(train_dir, cls))\n",
    "        os.makedirs(os.path.join(val_dir, cls))\n",
    "\n",
    "        images = os.listdir(os.path.join(root_folder, cls))\n",
    "        random.shuffle(images)\n",
    "\n",
    "        train_count = int(len(images) * train_ratio)\n",
    "        train_imgs = images[:train_count]\n",
    "        val_imgs = images[train_count:]\n",
    "\n",
    "        for img in train_imgs:\n",
    "            shutil.copy(os.path.join(root_folder, cls, img), os.path.join(train_dir, cls, img))\n",
    "        for img in val_imgs:\n",
    "            shutil.copy(os.path.join(root_folder, cls, img), os.path.join(val_dir, cls, img))\n",
    "\n",
    "    print(f\"‚úÖ Dataset split done! Train and val folders created at '{base_dir}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0e34d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset split done! Train and val folders created at 'split_data_EfficientNetB0'\n"
     ]
    }
   ],
   "source": [
    "# === Run the split ===\n",
    "create_train_val_split(root_folder, base_dir, train_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b3fb2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. Set up EfficientNet-B0 with Transfer Learning ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "edb12768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use EfficientNet's recommended weights and transforms\n",
    "weights = EfficientNet_B0_Weights.DEFAULT\n",
    "transform = weights.transforms()\n",
    "\n",
    "train_dataset = datasets.ImageFolder(os.path.join(base_dir, 'train'), transform=transform)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(base_dir, 'val'), transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0e280b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and modify classifier\n",
    "model_effb0 = efficientnet_b0(weights=weights)\n",
    "for param in model_effb0.parameters():\n",
    "    param.requires_grad = False  # freeze base\n",
    "\n",
    "num_features = model_effb0.classifier[1].in_features\n",
    "model_effb0.classifier[1] = nn.Linear(num_features, len(train_dataset.classes))  # replace final layer\n",
    "model_effb0 = model_effb0.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_effb0.classifier[1].parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68048dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. Training loop ===\n",
    "def train_model(epochs=10):\n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        model_effb0.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model_effb0(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        model_effb0.eval()\n",
    "        val_correct, val_total, val_loss = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model_effb0(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {avg_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    print(f\"\\nüèÜ Best Val Accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba2428f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.7220 | Train Acc: 46.88% | Val Loss: 0.7188 | Val Acc: 52.50%\n",
      "Epoch 2 | Train Loss: 0.5832 | Train Acc: 80.62% | Val Loss: 0.5940 | Val Acc: 80.00%\n",
      "Epoch 3 | Train Loss: 0.5243 | Train Acc: 82.50% | Val Loss: 0.5299 | Val Acc: 77.50%\n",
      "Epoch 4 | Train Loss: 0.4310 | Train Acc: 88.75% | Val Loss: 0.4793 | Val Acc: 85.00%\n",
      "Epoch 5 | Train Loss: 0.3971 | Train Acc: 89.38% | Val Loss: 0.4365 | Val Acc: 90.00%\n",
      "Epoch 6 | Train Loss: 0.3757 | Train Acc: 90.00% | Val Loss: 0.3980 | Val Acc: 92.50%\n",
      "Epoch 7 | Train Loss: 0.3282 | Train Acc: 94.38% | Val Loss: 0.3395 | Val Acc: 90.00%\n",
      "Epoch 8 | Train Loss: 0.3270 | Train Acc: 91.25% | Val Loss: 0.3073 | Val Acc: 90.00%\n",
      "Epoch 9 | Train Loss: 0.2740 | Train Acc: 95.62% | Val Loss: 0.2890 | Val Acc: 92.50%\n",
      "Epoch 10 | Train Loss: 0.2742 | Train Acc: 93.75% | Val Loss: 0.2793 | Val Acc: 87.50%\n",
      "\n",
      "üèÜ Best Val Accuracy: 92.50%\n"
     ]
    }
   ],
   "source": [
    "# === Run training ===\n",
    "train_model(epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ea87ca",
   "metadata": {},
   "source": [
    "Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb8347ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_image(image_path, model, class_names):\n",
    "    model.eval()\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        probs = F.softmax(output, dim=1)\n",
    "        _, predicted = torch.max(probs, 1)\n",
    "\n",
    "    print(f\"Predicted Class: {class_names[predicted.item()]}\")\n",
    "    print(f\"Class Probabilities: {probs.squeeze().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a18c19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: series_infected_leaves_augmented\n",
      "Class Probabilities: [0.16856721 0.83143276]\n"
     ]
    }
   ],
   "source": [
    "# Assuming dataset = ImageFolder(...)\n",
    "class_names = dataset.classes  # ['healthy', 'infected']\n",
    "\n",
    "# Path to one test image\n",
    "test_image_path_1 = \"processed_data/serie infected leaves/infected_05.png\"\n",
    "\n",
    "predict_single_image(test_image_path_1, model_effb0, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2af80ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: serie_healthy_leaves_augmented\n",
      "Class Probabilities: [0.7026029  0.29739708]\n"
     ]
    }
   ],
   "source": [
    "test_image_path_2 = \"processed_data/serie healthy leaves/healthy_05.png\"\n",
    "predict_single_image(test_image_path_2, model_effb0, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cddd4d3",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c83589ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Final Evaluation on Validation Set:\n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "  serie_healthy_leaves_augmented       0.89      0.85      0.87        20\n",
      "series_infected_leaves_augmented       0.86      0.90      0.88        20\n",
      "\n",
      "                        accuracy                           0.88        40\n",
      "                       macro avg       0.88      0.88      0.87        40\n",
      "                    weighted avg       0.88      0.88      0.87        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_final_model():\n",
    "    model_effb0.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model_effb0(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(\"\\nüìä Final Evaluation on Validation Set:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=val_dataset.classes, digits=2))\n",
    "\n",
    "# Run this after training\n",
    "evaluate_final_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54134d44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
