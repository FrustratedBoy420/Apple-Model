{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea5be075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms, models\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.optim import *\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a5b0edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = MobileNet_V2_Weights.DEFAULT\n",
    "model_mbvnet = mobilenet_v2(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77828e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze backbone\n",
    "for param in model_mbvnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24a77368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "transform = weights.transforms()\n",
    "base_dir = 'split_data_EfficientNetB0'\n",
    "train_dataset = datasets.ImageFolder(os.path.join(base_dir, 'train'), transform=transform)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(base_dir, 'val'), transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_mbvnet.classifier[1].parameters(), lr=0.001)\n",
    "dataset = datasets.ImageFolder('aug_processed_data', transform=transform)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c5e2019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace classifier\n",
    "num_features = model_mbvnet.classifier[1].in_features\n",
    "model_mbvnet.classifier[1] = nn.Linear(num_features, len(train_dataset.classes))\n",
    "model_mbvnet = model_mbvnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dbe3a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. Training loop ===\n",
    "def train_model(epochs=10):\n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        model_mbvnet.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model_mbvnet(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        model_mbvnet.eval()\n",
    "        val_correct, val_total, val_loss = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model_mbvnet(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {avg_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    print(f\"\\nüèÜ Best Val Accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3604cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.6989 | Train Acc: 48.75% | Val Loss: 0.7030 | Val Acc: 40.00%\n",
      "Epoch 2 | Train Loss: 0.6980 | Train Acc: 51.25% | Val Loss: 0.6813 | Val Acc: 45.00%\n",
      "Epoch 3 | Train Loss: 0.7003 | Train Acc: 50.62% | Val Loss: 0.6715 | Val Acc: 45.00%\n",
      "Epoch 4 | Train Loss: 0.6987 | Train Acc: 50.00% | Val Loss: 0.6637 | Val Acc: 50.00%\n",
      "Epoch 5 | Train Loss: 0.7066 | Train Acc: 45.62% | Val Loss: 0.6671 | Val Acc: 52.50%\n",
      "Epoch 6 | Train Loss: 0.7035 | Train Acc: 51.25% | Val Loss: 0.6764 | Val Acc: 55.00%\n",
      "Epoch 7 | Train Loss: 0.7012 | Train Acc: 49.38% | Val Loss: 0.6807 | Val Acc: 57.50%\n",
      "Epoch 8 | Train Loss: 0.7097 | Train Acc: 44.38% | Val Loss: 0.6841 | Val Acc: 60.00%\n",
      "Epoch 9 | Train Loss: 0.6989 | Train Acc: 47.50% | Val Loss: 0.6852 | Val Acc: 57.50%\n",
      "Epoch 10 | Train Loss: 0.7090 | Train Acc: 43.75% | Val Loss: 0.6874 | Val Acc: 57.50%\n",
      "\n",
      "üèÜ Best Val Accuracy: 60.00%\n"
     ]
    }
   ],
   "source": [
    "# === Run training ===\n",
    "train_model(epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9e9870",
   "metadata": {},
   "source": [
    "Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "162a3a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_image(image_path, model, class_names):\n",
    "    model.eval()\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        probs = F.softmax(output, dim=1)\n",
    "        _, predicted = torch.max(probs, 1)\n",
    "\n",
    "    print(f\"Predicted Class: {class_names[predicted.item()]}\")\n",
    "    print(f\"Class Probabilities: {probs.squeeze().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfe04d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: series_infected_leaves_augmented\n",
      "Class Probabilities: [0.4445408 0.5554592]\n"
     ]
    }
   ],
   "source": [
    "# Assuming dataset = ImageFolder(...)\n",
    "class_names = dataset.classes  # ['healthy', 'infected']\n",
    "\n",
    "# Path to one test image\n",
    "test_image_path_1 = \"processed_data/serie infected leaves/infected_05.png\"\n",
    "\n",
    "predict_single_image(test_image_path_1, model_mbvnet, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db28a59",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "959bdebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Final Evaluation on Validation Set:\n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "  serie_healthy_leaves_augmented       0.57      0.60      0.59        20\n",
      "series_infected_leaves_augmented       0.58      0.55      0.56        20\n",
      "\n",
      "                        accuracy                           0.57        40\n",
      "                       macro avg       0.58      0.57      0.57        40\n",
      "                    weighted avg       0.58      0.57      0.57        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_final_model():\n",
    "    model_mbvnet.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model_mbvnet(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(\"\\nüìä Final Evaluation on Validation Set:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=val_dataset.classes, digits=2))\n",
    "\n",
    "# Run this after training\n",
    "evaluate_final_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d73c07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
