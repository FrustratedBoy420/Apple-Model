{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ea5be075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms, models\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.optim import *\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a5b0edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = MobileNet_V2_Weights.DEFAULT\n",
    "model_mbvnet = mobilenet_v2(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e4b916d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder('aug_processed_data', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77828e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze backbone\n",
    "for param in model_mbvnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24a77368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "transform = weights.transforms()\n",
    "base_dir = 'split_data_EfficientNetB0'\n",
    "train_dataset = datasets.ImageFolder(os.path.join(base_dir, 'train'), transform=transform)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(base_dir, 'val'), transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_mbvnet.classifier[1].parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c5e2019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace classifier\n",
    "num_features = model_mbvnet.classifier[1].in_features\n",
    "model_mbvnet.classifier[1] = nn.Linear(num_features, len(train_dataset.classes))\n",
    "model_mbvnet = model_mbvnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4dbe3a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. Training loop ===\n",
    "def train_model(epochs=10):\n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        model_mbvnet.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model_mbvnet(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        model_mbvnet.eval()\n",
    "        val_correct, val_total, val_loss = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model_mbvnet(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {avg_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    print(f\"\\nüèÜ Best Val Accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3604cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.6992 | Train Acc: 47.50% | Val Loss: 0.6459 | Val Acc: 47.50%\n",
      "Epoch 2 | Train Loss: 0.7101 | Train Acc: 43.75% | Val Loss: 0.6420 | Val Acc: 60.00%\n",
      "Epoch 3 | Train Loss: 0.7002 | Train Acc: 52.50% | Val Loss: 0.6416 | Val Acc: 65.00%\n",
      "Epoch 4 | Train Loss: 0.6963 | Train Acc: 56.25% | Val Loss: 0.6429 | Val Acc: 65.00%\n",
      "Epoch 5 | Train Loss: 0.6900 | Train Acc: 51.88% | Val Loss: 0.6441 | Val Acc: 62.50%\n",
      "Epoch 6 | Train Loss: 0.6976 | Train Acc: 50.62% | Val Loss: 0.6468 | Val Acc: 55.00%\n",
      "Epoch 7 | Train Loss: 0.6984 | Train Acc: 52.50% | Val Loss: 0.6502 | Val Acc: 47.50%\n",
      "Epoch 8 | Train Loss: 0.6942 | Train Acc: 51.25% | Val Loss: 0.6532 | Val Acc: 45.00%\n",
      "Epoch 9 | Train Loss: 0.6964 | Train Acc: 54.38% | Val Loss: 0.6565 | Val Acc: 45.00%\n",
      "Epoch 10 | Train Loss: 0.7021 | Train Acc: 52.50% | Val Loss: 0.6560 | Val Acc: 45.00%\n",
      "\n",
      "üèÜ Best Val Accuracy: 65.00%\n"
     ]
    }
   ],
   "source": [
    "# === Run training ===\n",
    "train_model(epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9e9870",
   "metadata": {},
   "source": [
    "Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "162a3a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_image(image_path, model, class_names):\n",
    "    model.eval()\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        probs = F.softmax(output, dim=1)\n",
    "        _, predicted = torch.max(probs, 1)\n",
    "\n",
    "    print(f\"Predicted Class: {class_names[predicted.item()]}\")\n",
    "    print(f\"Class Probabilities: {probs.squeeze().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bfe04d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: series_infected_leaves_augmented\n",
      "Class Probabilities: [0.44847256 0.55152744]\n"
     ]
    }
   ],
   "source": [
    "# Assuming dataset = ImageFolder(...)\n",
    "class_names = dataset.classes  # ['healthy', 'infected']\n",
    "\n",
    "# Path to one test image\n",
    "test_image_path_1 = \"processed_data/serie infected leaves/infected_05.png\"\n",
    "\n",
    "predict_single_image(test_image_path_1, model_mbvnet, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db28a59",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "959bdebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Final Evaluation on Validation Set:\n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "  serie_healthy_leaves_augmented       0.40      0.20      0.27        20\n",
      "series_infected_leaves_augmented       0.47      0.70      0.56        20\n",
      "\n",
      "                        accuracy                           0.45        40\n",
      "                       macro avg       0.43      0.45      0.41        40\n",
      "                    weighted avg       0.43      0.45      0.41        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_final_model():\n",
    "    model_mbvnet.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model_mbvnet(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(\"\\nüìä Final Evaluation on Validation Set:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=val_dataset.classes, digits=2))\n",
    "\n",
    "# Run this after training\n",
    "evaluate_final_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d73c07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
